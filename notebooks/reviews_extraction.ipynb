{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Data Collection and Processing\n",
    "### Objectives :\n",
    "- Scrape \"Backmarket\" customers reviews from Trustpilot.\n",
    "- Clean review data collected\n",
    "- Processed reviews data collected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Logging Configuration\n",
    "- All log messages (INFO, ERROR, etc.) will be written to the specified log file (concatenation.log).\n",
    "- If StreamHandler is included, logs will also appear in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the logs directory exists\n",
    "log_dir = \"../logs_etl\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "log_file = os.path.join(log_dir, \"concatenation.log\")\n",
    "\n",
    "# Configure logging only if it hasn't been configured yet\n",
    "if not logging.root.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,  # Fixed logging level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),  # Write logs to a file\n",
    "            logging.StreamHandler()         # Optional: Keep console output\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "Target company for review analysis : Back Market - A global marketplace for refurbished devices. \n",
    "\n",
    "Review data will be collected from Trustpilot, a platform for collecting verified customer reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and parameters\n",
    "COMPANY_NAME = \"backmarket\" # Backmarket\n",
    "BASE_URL = f'https://fr.trustpilot.com/review/www.{COMPANY_NAME}.fr'\n",
    "MAX_PAGES = 650\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to scrape reviews from trustpilot\n",
    "This Python function, extract_reviews, is designed to scrape and collect customer reviews from a paginated website ([trustpilot platform](https://fr.trustpilot.com/)) for a specific company. \n",
    "Here's a concise description of its functionality:\n",
    "\n",
    "1. **Purpose** :\n",
    "The function extracts structured review data (e.g., review ID, title, text, rating, reply, dates) from multiple pages of a website and saves the collected data into a CSV file.\n",
    "\n",
    "2. **Key Steps** :\n",
    "    - Input Parameters :\n",
    "        - company_name: Name of the company (default is a predefined constant).\n",
    "        - start_page: The page number to start scraping from (default is 1).\n",
    "        - end_page: The last page to scrape (default is a predefined constant).\n",
    "    - HTTP Requests :\n",
    "        Sends GET requests to the website using requests with custom headers and handles potential HTTP errors.\n",
    "    - HTML Parsing :\n",
    "        Uses BeautifulSoup to parse the HTML content and extract embedded JSON data containing reviews from a script tag.\n",
    "    - Data Extraction :\n",
    "        Iterates through the JSON data, extracting specific fields (e.g., review text, rating, dates) and storing them in a list of dictionaries.\n",
    "    -  Error Handling :\n",
    "        Includes error handling for network issues, missing data, and parsing errors.\n",
    "    - Rate Limiting :\n",
    "        Implements a delay (time.sleep) after processing every 10 pages to avoid overloading the server.\n",
    "    - Output :\n",
    "    Converts the collected data into a Pandas DataFrame and saves it as a CSV file in a specified directory.\n",
    "3. **Output** :\n",
    "    - Returns the file path of the saved CSV file containing the extracted reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(company_name=COMPANY_NAME, start_page=1, end_page=2):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from a paginated website for a specific company. \n",
    "    Extracts key review details such as text, rating, and dates into a structured format. \n",
    "    Saves the collected data as a CSV file for further analysis.\n",
    "    \"\"\"\n",
    "    headers = HEADERS\n",
    "    base_url = BASE_URL\n",
    "    keys = [\"id\", \"title\", \"review\", \"rating\", \"reply\", \"experienceDate\", \"createdDateTime\", \"publishedDate\"]\n",
    "    \n",
    "    reviews_list = []  # List to collect all review data\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        logging.info(f\"Processing page {page}\")\n",
    "        \n",
    "        url_page = f\"{base_url}?page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url_page, headers=headers, timeout=5)\n",
    "            response.raise_for_status()  # Verify HTTP errors\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error accessing page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            script_content = soup.body.script.contents if soup.body and soup.body.script else None\n",
    "            \n",
    "            if not script_content:\n",
    "                logging.warning(f\"No data found in page {page}\")\n",
    "                continue\n",
    "\n",
    "            raw_data = json.loads(script_content[0])\n",
    "            raw_data = raw_data.get(\"props\", {}).get(\"pageProps\", {}).get(\"reviews\", [])\n",
    "            \n",
    "            for review in raw_data:\n",
    "                tmp = {}\n",
    "                tmp[\"id\"] = review.get(\"id\")\n",
    "                tmp[\"title\"] = review.get(\"title\")\n",
    "                tmp[\"review\"] = review.get(\"text\")\n",
    "                tmp[\"rating\"] = review.get(\"rating\")\n",
    "                try:\n",
    "                    tmp[\"reply\"] = review.get(\"reply\", {}).get(\"message\")\n",
    "                    tmp[\"replyPublishedDate\"] = review.get(\"reply\", {}).get(\"publishedDate\")\n",
    "                except:\n",
    "                    tmp[\"reply\"] = None\n",
    "                    tmp[\"replyPublishedDate\"] = None\n",
    "                \n",
    "                tmp[\"experienceDate\"] = review.get(\"dates\", {}).get(\"experiencedDate\")\n",
    "                tmp[\"createdDateTime\"] = review.get(\"labels\", {}).get(\"verification\", {}).get(\"createdDateTime\")\n",
    "                tmp[\"publishedDate\"] = review.get(\"dates\", {}).get(\"publishedDate\")\n",
    "\n",
    "                reviews_list.append({key: tmp.get(key) for key in keys})\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing page {page}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Avoid hitting the server too frequently\n",
    "        if page > 10 and page % 10 == 0:\n",
    "            logging.info(\"Sleeping for 100 seconds to avoid overloading the server.\")\n",
    "            time.sleep(100)\n",
    "\n",
    "    if not reviews_list:\n",
    "        logging.warning(\"No reviews collected.\")\n",
    "        return\n",
    "    \n",
    "    # Convert list of dicts to DataFrame and save to CSV\n",
    "    df_raw_reviews = pd.DataFrame(reviews_list)\n",
    "\n",
    "    # Save reviews data into a CSV file\n",
    "    output_dir = \"../data/raw\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    raw_file_path = os.path.join(output_dir, f\"raw_reviews_{start_page}-{end_page}.csv\")\n",
    "    df_raw_reviews.to_csv(raw_file_path, index=False)\n",
    "    logging.info(f\"Saved reviews data to {raw_file_path}\")\n",
    "    return raw_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "extract_reviews(company_name=COMPANY_NAME, start_page=1, end_page=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data collection\n",
    "There are over 3,000 review pages for Back Market, making it time-consuming to scrape all the data. To avoid overloading the server, we ran the extraction function in batches of 25 pages, incorporating significant delays between batches. The initial scraped data has been saved in the \"data/raw\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all extracted reviews into one csv file\n",
    "def concatenate_reviews(input_dir=\"../data/raw\", \\\n",
    "                        output_file=\"raw_reviews_0.csv\", \\\n",
    "                        file_prefix=\"raw_reviews_\"):\n",
    "    \"\"\"\n",
    "    Concatenates multiple review CSV files into a single DataFrame,\n",
    "    removes duplicates and missing values, \n",
    "    saves the cleaned data to a new CSV file,\n",
    "    and moves processed files to an '.archive' folder.\n",
    "    \"\"\"\n",
    "    # Validate input directory\n",
    "    if not os.path.exists(input_dir):\n",
    "        logging.error(f\"Input directory '{input_dir}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # List files matching the prefix\n",
    "    files = [f for f in os.listdir(input_dir) if f.startswith(file_prefix)]\n",
    "    if not files:\n",
    "        logging.error(f\"No files found with prefix '{file_prefix}' in '{input_dir}'.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Found {len(files)} files to process.\")\n",
    "\n",
    "    # Create .archive folder if it doesn't exist\n",
    "    archive_dir = os.path.join(input_dir, \".archive\")\n",
    "    os.makedirs(archive_dir, exist_ok=True)\n",
    "    logging.info(f\"Created/verified '.archive' folder at: {archive_dir}\")\n",
    "\n",
    "    # Read and concatenate files efficiently\n",
    "    try:\n",
    "        df_list = []\n",
    "        for f in files:\n",
    "            file_path = os.path.join(input_dir, f)\n",
    "            logging.info(f\"Reading file: {file_path}\")\n",
    "            df_list.append(pd.read_csv(file_path))\n",
    "        \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Concatenated all files into a single DataFrame.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading or concatenating files: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Log initial state\n",
    "    logging.info(f\"Initial DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "    logging.info(f\"Removed duplicates. New DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Drop rows with missing values in critical columns\n",
    "    critical_columns = [\"id\", \"review\", \"rating\", \"experienceDate\"]\n",
    "    df.dropna(subset=critical_columns, inplace=True)\n",
    "    logging.info(f\"Removed rows with missing values. Final DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Save cleaned DataFrame to CSV\n",
    "    try:\n",
    "        output_path = os.path.join(input_dir, output_file)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Saved cleaned DataFrame to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cleaned DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Move processed files to .archive folder\n",
    "    try:\n",
    "        for f in files:\n",
    "            src_path = os.path.join(input_dir, f)\n",
    "            dst_path = os.path.join(archive_dir, f)\n",
    "            shutil.move(src_path, dst_path)\n",
    "            logging.info(f\"Moved file to archive: {dst_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error moving files to archive: {e}\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all extracted reviews into one csv file\n",
    "df = concatenate_reviews()\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process reviews data\n",
    "\n",
    "In this step we preprocess raw text data for Natural Language Processing (NLP) tasks by performing a series of cleaning and normalization steps. We removes noise (e.g., hashtags, URLs, mentions, stopwords), converts text to lowercase, tokenizes it, filters out non-alphabetic tokens, and returns the cleaned text as a single string.\n",
    "\n",
    "#### Text cleaning and normalization\n",
    "- Noise Removal :\n",
    "    - Removes hashtags, HTML entities, stock tickers, URLs, retweet tags, mentions, and special characters.\n",
    "    - Replaces emojis with their textual descriptions and strips punctuation.\n",
    "\n",
    "- Normalization :\n",
    "    - Converts text to lowercase, replaces ampersands (&) with \"and,\" and removes short words (â‰¤3 characters).\n",
    "\n",
    "- Tokenization :\n",
    "    - Splits text into tokens using French-specific word tokenization.\n",
    "\n",
    "- Filtering :\n",
    "    - Removes numbers, non-alphabetic tokens, and stopwords (including custom additions to the French stopwords list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love this product great fast service\n"
     ]
    }
   ],
   "source": [
    "# Function to clean and process review text\n",
    "\n",
    "# os.environ[\"NLTK_DATA\"] = \"/Users/micheldpd/Projects/custrev/nltk_data\"\n",
    "\n",
    "# Preload stopwords outside the function to avoid repeated loading\n",
    "STOP_WORDS_TO_ADD = [\"Ãªtre\", \"leur\", \"leurs\", \"avoir\", \"cela\", \"les\", \"de\", \"pour\", \"des\", \"cette\", \"a\",\n",
    "                   \"j'ai\", \"car\", \"c'est\", \"chez\", \"tout\", \"fait\", \"chez\", \"donc\", \n",
    "                   \"n'est\", \"si\", \"alors\", \"n'ai\", \"faire\", \"deux\", \"comme\", \"jour\", \"tr\", \"si\", \"ue\"\n",
    "\n",
    "]\n",
    "STOP_WORDS = set(stopwords.words('french')).union(set(STOP_WORDS_TO_ADD))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing noise (e.g., hashtags, URLs, stopwords) and normalizing content.\n",
    "    Tokenizes, filters alphabetic tokens, and removes French stopwords for NLP tasks.\n",
    "    Returns the cleaned and normalized text as a single string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove hashtags (keep text after #)\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Remove HTML special entities (e.g., &amp;)\n",
    "    text = re.sub(r'&\\w*;', '', text)\n",
    "\n",
    "    # Remove stock tickers (e.g., $AAPL)\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "\n",
    "    # Remove hyperlinks (covers various URL patterns)\n",
    "    text = re.sub(r'https?://[^\\s/$.?#].[^\\s]*', '', text)\n",
    "    text = re.sub(r'http(\\S)+', '', text)  # Catch incomplete URLs\n",
    "    text = re.sub(r'http\\s*\\.\\.\\.', '', text)  # Catch truncated URLs\n",
    "\n",
    "    # Remove retweet tags and mentions\n",
    "    text = re.sub(r'(RT|rt)\\s*@\\s*\\S+', '', text)\n",
    "    text = re.sub(r'RT\\s?@', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "    # Replace & with 'and', fix < and > (assuming intent was to escape them)\n",
    "    text = re.sub(r'&', 'and', text)\n",
    "\n",
    "    # Remove words with 3 or fewer letters (e.g., \"the\", \"cat\")\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', ' ', text)\n",
    "\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode\n",
    "    text = ''.join(c for c in text if ord(c) <= 0xFFFF)\n",
    "\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert emojis to text descriptions (e.g., ðŸ˜Š -> :smiling_face:)\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove punctuation, keeping alphanumeric characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Tokenize text (lowercase for consistency)\n",
    "    tokens: List[str] = word_tokenize(text.lower(), language='french')\n",
    "\n",
    "    # Filter out numbers and keep only alphabetic tokens\n",
    "    tokens_alpha = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens_cleaned = [token for token in tokens_alpha if token not in STOP_WORDS]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens_cleaned)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"RT @user: I love this product! < and 16Â§789> #great https://example.com ðŸ˜Š &amp; fast service\"\n",
    "    cleaned = clean_text(sample_text)\n",
    "    print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review data cleaning and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define customer sentiment\n",
    "- Customer sentiment is \"positive\" if rating >= 4\n",
    "- Customer sentiment is neutral if rating == 3\n",
    "- Customer sentiment is \"negative if rating < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define the sentiment base on the rating value\n",
    "\n",
    "def get_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return \"positive\"\n",
    "    elif rating == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_reviews(raw_file):\n",
    "    \"\"\"\n",
    "    Processes raw review data by standardizing dates, extracting temporal features, and cleaning text.\n",
    "    Removes rows with missing values, invalid ratings, or short reviews, ensuring data quality.\n",
    "    Saves the cleaned and filtered reviews to a timestamped CSV file for further analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load raw data\n",
    "        logging.info(f\"Loading raw data from: {raw_file}\")\n",
    "        df = pd.read_csv(raw_file)\n",
    "        logging.info(f\"Successfully loaded {len(df)} rows of data.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading raw data: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Standardize date formats\n",
    "        logging.info(\"Standardizing date formats...\")\n",
    "        df[\"experienceDate\"] = pd.to_datetime(df[\"experienceDate\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df[\"createdDateTime\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        try:\n",
    "            df[\"replyPublishedDate\"] = pd.to_datetime(df[\"replyPublishedDate\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except:\n",
    "            df[\"replyPublishedDate\"] = None\n",
    "        logging.info(\"Date formats standardized successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error standardizing date formats: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract temporal features\n",
    "        logging.info(\"Extracting temporal features (year, month, day, hour)...\")\n",
    "        df['year'] = pd.to_datetime(df['createdDateTime']).dt.year\n",
    "        df[\"yearQuarter\"] = pd.to_datetime(df['createdDateTime']).dt.year.astype(str) + \\\n",
    "            \"-Q\" + pd.to_datetime(df['createdDateTime']).dt.quarter.astype(str)\n",
    "        df['month'] = pd.to_datetime(df['createdDateTime']).dt.month\n",
    "        df['monthName'] = pd.to_datetime(df['createdDateTime']).dt.month_name()\n",
    "        df['day'] = pd.to_datetime(df['createdDateTime']).dt.day\n",
    "        df['dayName'] = pd.to_datetime(df['createdDateTime']).dt.day_name()\n",
    "        df['hour'] = pd.to_datetime(df['createdDateTime']).dt.hour\n",
    "        try:\n",
    "            df['replyYear'] = pd.to_datetime(df['replyPublishedDate']).dt.year\n",
    "            df['replyMonth'] = pd.to_datetime(df['replyPublishedDate']).dt.month\n",
    "            df['replyDay'] = pd.to_datetime(df['replyPublishedDate']).dt.day\n",
    "            df['replyHour'] = pd.to_datetime(df['replyPublishedDate']).dt.hour\n",
    "        except:\n",
    "            df['replyYear'] = None\n",
    "            df['replyMonth'] = None\n",
    "            df['replyDay'] = None\n",
    "            df['replyHour'] = None\n",
    "        logging.info(\"Temporal features extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting temporal features: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Remove rows with missing values\n",
    "        logging.info(\"Removing rows with missing values...\")\n",
    "        initial_rows = len(df)\n",
    "        df.dropna(inplace=True, subset=[\"id\", \"review\", \"rating\", \"experienceDate\", \"createdDateTime\", \"publishedDate\"])\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} rows with missing values. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing rows with missing values: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Remove rows with invalid ratings\n",
    "        logging.info(\"Removing rows with invalid ratings...\")\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"rating\"].isin([1, 2, 3, 4, 5])]\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} rows with invalid ratings. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing rows with invalid ratings: {e}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Add sentiment column\n",
    "        logging.info(\"Adding sentiment column...\")\n",
    "        df[\"sentiment\"] = df[\"rating\"].apply(get_sentiment)\n",
    "        logging.info(\"Sentiment column added successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error adding sentiment column: {e}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Clean review text\n",
    "        logging.info(\"Cleaning review text...\")\n",
    "        df[\"review\"] = df[\"review\"].apply(clean_text)\n",
    "        logging.info(\"Review text cleaned successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning review text: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Remove short reviews\n",
    "        logging.info(\"Removing short reviews...\")\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"review\"].str.len() > 4]\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} short reviews. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing short reviews: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Save cleaned data to CSV\n",
    "        logging.info(\"Saving cleaned data to CSV...\")\n",
    "        output_dir = \"../data/cleaned\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        cleaned_file_path = os.path.join(output_dir, f\"cleaned_reviews_{timestamp}.csv\")\n",
    "        df.to_csv(cleaned_file_path, index=False)\n",
    "        logging.info(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cleaned data: {e}\")\n",
    "        return None\n",
    "\n",
    "    return cleaned_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed raw review\n",
    "process_reviews(\"../data/raw/raw_reviews_0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading cleaned review into the full review base\n",
    "\n",
    "After cleaning and processing, the extracted raw reviews are consolidated into the full review database.\n",
    "Before consolidation, the pipeline checks for the presence of new data. If no new data is found, the full review database remains unchanged, and the pipeline stops at this stage.\n",
    "Once the loading process is complete, the cleaned review file is archived to ensure proper organization and to maintain a history of processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(cleaned_file) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes review data by merging new data with existing cleaned data.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_file (str): Path to the new review data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame after merging and deduplication\n",
    "        None: If an error occurs during processing or if no new data is added\n",
    "        \n",
    "    Key Features:\n",
    "        - Ensures output directory exists\n",
    "        - Handles existing data loading with error checking\n",
    "        - Updates data only if new reviews are added\n",
    "        - Writes \"1\" to ../parameters/new_reviews.txt if new reviews exist, otherwise \"0\"\n",
    "        - Logs percentage increase in data when new reviews are added\n",
    "        - Saves updated data with timestamp backup only if necessary\n",
    "        - Archives the cleaned review file after processing\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load new review data\n",
    "        df = pd.read_csv(cleaned_file)\n",
    "\n",
    "        # Define paths\n",
    "        base_dir = \"../data\"\n",
    "        full_reviews_folder = os.path.join(base_dir, \"full\")\n",
    "        archive_folder = os.path.join(base_dir, \"archive\")  # Archive folder for processed files\n",
    "        full_reviews_file = \"full_reviews.csv\"\n",
    "        full_reviews_path = os.path.join(full_reviews_folder, full_reviews_file)\n",
    "        new_reviews_flag_path = os.path.join(\"../parameters\", \"new_reviews.txt\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(full_reviews_folder, exist_ok=True)\n",
    "        os.makedirs(archive_folder, exist_ok=True)  # Create archive folder\n",
    "        os.makedirs(os.path.dirname(new_reviews_flag_path), exist_ok=True)\n",
    "        logging.info(f\"Directories ensured: {full_reviews_folder}, {archive_folder}, {os.path.dirname(new_reviews_flag_path)}\")\n",
    "\n",
    "        # Load existing data with error handling\n",
    "        df_full = pd.DataFrame()\n",
    "        if os.path.isfile(full_reviews_path):\n",
    "            try:\n",
    "                df_full = pd.read_csv(full_reviews_path, low_memory=False)\n",
    "                logging.info(f\"Loaded existing data from {full_reviews_path}\")\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.warning(f\"Empty CSV file found at {full_reviews_path}\")\n",
    "            except pd.errors.ParserError:\n",
    "                logging.error(f\"Parsing error in {full_reviews_path}\")\n",
    "        else:\n",
    "            logging.info(f\"No existing file found at {full_reviews_path}, initializing empty DataFrame\")\n",
    "\n",
    "        # Validate input DataFrame\n",
    "        if df.empty:\n",
    "            logging.warning(\"Input DataFrame is empty\")\n",
    "            return None\n",
    "\n",
    "        # Merge and deduplicate\n",
    "        initial_length = len(df_full)\n",
    "        df_full_updated = pd.concat([df_full, df], ignore_index=True)\n",
    "        df_full_updated = df_full_updated.drop_duplicates(subset=df.columns, keep='last')\n",
    "        final_length = len(df_full_updated)\n",
    "\n",
    "        # Log DataFrame sizes\n",
    "        logging.info(f\"Original records: {initial_length}\")\n",
    "        logging.info(f\"Updated records: {final_length}\")\n",
    "        new_records_added = final_length - initial_length\n",
    "        logging.info(f\"New records added: {new_records_added}\")\n",
    "\n",
    "        # Check if there are new records\n",
    "        has_new_reviews = final_length > initial_length\n",
    "        flag_value = \"1\" if has_new_reviews else \"0\"\n",
    "        with open(new_reviews_flag_path, \"w\") as f:\n",
    "            f.write(flag_value)\n",
    "        logging.info(f\"Wrote '{flag_value}' to {new_reviews_flag_path}\")\n",
    "\n",
    "        # Exit early if no new reviews are added\n",
    "        if not has_new_reviews:\n",
    "            logging.info(\"No new reviews to process. Exiting without updates.\")\n",
    "            return df_full_updated\n",
    "\n",
    "        # Calculate percentage increase\n",
    "        if initial_length == 0 and final_length > 0:\n",
    "            percentage_increase = 100\n",
    "        elif initial_length > 0:\n",
    "            percentage_increase = ((final_length - initial_length) / initial_length * 100) \n",
    "        else: percentage_increase = 0\n",
    "        logging.info(f\"Percentage increase in data: {percentage_increase:.2f}%\")\n",
    "\n",
    "        # Create backup before overwriting\n",
    "        if os.path.isfile(full_reviews_path):\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_path = os.path.join(full_reviews_folder, f\"full_reviews_backup_{timestamp}.csv\")\n",
    "            df_full.to_csv(backup_path, index=False)\n",
    "            logging.info(f\"Backup created at {backup_path}\")\n",
    "\n",
    "        # Save updated data\n",
    "        df_full_updated.to_csv(full_reviews_path, index=False)\n",
    "        logging.info(f\"Updated data saved to {full_reviews_path}\")\n",
    "\n",
    "        # Archive the cleaned review file\n",
    "        try:\n",
    "            archive_path = os.path.join(archive_folder, os.path.basename(cleaned_file))\n",
    "            shutil.move(cleaned_file, archive_path)\n",
    "            logging.info(f\"Archived cleaned review file to: {archive_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error archiving cleaned review file: {e}\")\n",
    "\n",
    "        return df_full_updated\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing reviews: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 21:29:10,911 - INFO - Processing page 3000\n",
      "2025-03-26 21:29:11,390 - INFO - Sleeping for 100 seconds to avoid overloading the server.\n",
      "2025-03-26 21:30:51,395 - INFO - Processing page 3001\n",
      "2025-03-26 21:30:52,094 - INFO - Processing page 3002\n",
      "2025-03-26 21:30:53,080 - INFO - Processing page 3003\n",
      "2025-03-26 21:30:53,808 - INFO - Processing page 3004\n",
      "2025-03-26 21:30:54,659 - INFO - Processing page 3005\n",
      "2025-03-26 21:30:55,210 - INFO - Processing page 3006\n",
      "2025-03-26 21:30:56,324 - INFO - Processing page 3007\n",
      "2025-03-26 21:30:57,282 - INFO - Processing page 3008\n",
      "2025-03-26 21:30:57,960 - INFO - Processing page 3009\n",
      "2025-03-26 21:30:59,372 - INFO - Processing page 3010\n",
      "2025-03-26 21:30:59,930 - INFO - Sleeping for 100 seconds to avoid overloading the server.\n"
     ]
    }
   ],
   "source": [
    "# application\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    raw_file_path = extract_reviews(company_name=COMPANY_NAME, start_page=3000, end_page=3250)\n",
    "    print(\"\\n transformed reviews: \\n\")\n",
    "    cleaned_file = process_reviews(raw_file_path)\n",
    "    print(f\"\\n cleaned_file_path: {cleaned_file} \\n\")\n",
    "\n",
    "    df_clean = pd.read_csv(cleaned_file)\n",
    "    print(df_clean.info())\n",
    "    print(df_clean.head())\n",
    "\n",
    "    print(\"\\n load reviews: \\n\")\n",
    "    load_reviews(cleaned_file)\n",
    "    df_full = pd.read_csv(\"../data/full/full_reviews.csv\")\n",
    "    print(df_full.info())\n",
    "    print(df_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/full/full_reviews.csv\")\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
