{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Data Collection and Processing\n",
    "### Objectives :\n",
    "- Scrape \"Backmarket\" customers reviews from Trustpilot.\n",
    "- Clean review data collected\n",
    "- Processed reviews data collected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Logging Configuration\n",
    "- All log messages (INFO, ERROR, etc.) will be written to the specified log file (concatenation.log).\n",
    "- If StreamHandler is included, logs will also appear in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the logs directory exists\n",
    "log_dir = \"../logs_etl\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "log_file = os.path.join(log_dir, \"concatenation.log\")\n",
    "\n",
    "# Configure logging only if it hasn't been configured yet\n",
    "if not logging.root.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,  # Fixed logging level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),  # Write logs to a file\n",
    "            logging.StreamHandler()         # Optional: Keep console output\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "Target company for review analysis : Back Market - A global marketplace for refurbished devices. \n",
    "\n",
    "Review data will be collected from Trustpilot, a platform for collecting verified customer reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and parameters\n",
    "COMPANY_NAME = \"backmarket\" # Backmarket\n",
    "BASE_URL = f'https://fr.trustpilot.com/review/www.{COMPANY_NAME}.fr'\n",
    "MAX_PAGES = 650\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to scrape reviews from trustpilot\n",
    "This Python function, extract_reviews, is designed to scrape and collect customer reviews from a paginated website ([trustpilot platform](https://fr.trustpilot.com/)) for a specific company. \n",
    "Here's a concise description of its functionality:\n",
    "\n",
    "1. **Purpose** :\n",
    "The function extracts structured review data (e.g., review ID, title, text, rating, reply, dates) from multiple pages of a website and saves the collected data into a CSV file.\n",
    "\n",
    "2. **Key Steps** :\n",
    "    - Input Parameters :\n",
    "        - company_name: Name of the company (default is a predefined constant).\n",
    "        - start_page: The page number to start scraping from (default is 1).\n",
    "        - end_page: The last page to scrape (default is a predefined constant).\n",
    "    - HTTP Requests :\n",
    "        Sends GET requests to the website using requests with custom headers and handles potential HTTP errors.\n",
    "    - HTML Parsing :\n",
    "        Uses BeautifulSoup to parse the HTML content and extract embedded JSON data containing reviews from a script tag.\n",
    "    - Data Extraction :\n",
    "        Iterates through the JSON data, extracting specific fields (e.g., review text, rating, dates) and storing them in a list of dictionaries.\n",
    "    -  Error Handling :\n",
    "        Includes error handling for network issues, missing data, and parsing errors.\n",
    "    - Rate Limiting :\n",
    "        Implements a delay (time.sleep) after processing every 10 pages to avoid overloading the server.\n",
    "    - Output :\n",
    "    Converts the collected data into a Pandas DataFrame and saves it as a CSV file in a specified directory.\n",
    "3. **Output** :\n",
    "    - Returns the file path of the saved CSV file containing the extracted reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(company_name=COMPANY_NAME, start_page=1, end_page=2):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from a paginated website for a specific company. \n",
    "    Extracts key review details such as text, rating, and dates into a structured format. \n",
    "    Saves the collected data as a CSV file for further analysis.\n",
    "    \"\"\"\n",
    "    headers = HEADERS\n",
    "    base_url = BASE_URL\n",
    "    keys = [\"id\", \"title\", \"review\", \"rating\", \"reply\", \"experienceDate\", \"createdDateTime\", \"publishedDate\"]\n",
    "    \n",
    "    reviews_list = []  # List to collect all review data\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        logging.info(f\"Processing page {page}\")\n",
    "        \n",
    "        url_page = f\"{base_url}?page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url_page, headers=headers, timeout=5)\n",
    "            response.raise_for_status()  # Verify HTTP errors\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error accessing page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            script_content = soup.body.script.contents if soup.body and soup.body.script else None\n",
    "            \n",
    "            if not script_content:\n",
    "                logging.warning(f\"No data found in page {page}\")\n",
    "                continue\n",
    "\n",
    "            raw_data = json.loads(script_content[0])\n",
    "            raw_data = raw_data.get(\"props\", {}).get(\"pageProps\", {}).get(\"reviews\", [])\n",
    "            \n",
    "            for review in raw_data:\n",
    "                tmp = {}\n",
    "                tmp[\"id\"] = review.get(\"id\")\n",
    "                tmp[\"title\"] = review.get(\"title\")\n",
    "                tmp[\"review\"] = review.get(\"text\")\n",
    "                tmp[\"rating\"] = review.get(\"rating\")\n",
    "                try:\n",
    "                    tmp[\"reply\"] = review.get(\"reply\", {}).get(\"message\")\n",
    "                except:\n",
    "                    tmp[\"reply\"] = None\n",
    "                \n",
    "                tmp[\"experienceDate\"] = review.get(\"dates\", {}).get(\"experiencedDate\")\n",
    "                tmp[\"createdDateTime\"] = review.get(\"labels\", {}).get(\"verification\", {}).get(\"createdDateTime\")\n",
    "                tmp[\"publishedDate\"] = review.get(\"dates\", {}).get(\"publishedDate\")\n",
    "\n",
    "                reviews_list.append({key: tmp.get(key) for key in keys})\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing page {page}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Avoid hitting the server too frequently\n",
    "        if page > 10 and page % 10 == 0:\n",
    "            logging.info(\"Sleeping for 100 seconds to avoid overloading the server.\")\n",
    "            time.sleep(100)\n",
    "\n",
    "    if not reviews_list:\n",
    "        logging.warning(\"No reviews collected.\")\n",
    "        return\n",
    "    \n",
    "    # Convert list of dicts to DataFrame and save to CSV\n",
    "    df_raw_reviews = pd.DataFrame(reviews_list)\n",
    "\n",
    "    # Save reviews data into a CSV file\n",
    "    output_dir = \"../data/raw\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    raw_file_path = os.path.join(output_dir, f\"raw_reviews_{start_page}-{end_page}.csv\")\n",
    "    df_raw_reviews.to_csv(raw_file_path, index=False)\n",
    "    logging.info(f\"Saved reviews data to {raw_file_path}\")\n",
    "    return raw_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:47:34,027 - INFO - Processing page 1\n",
      "2025-03-26 18:47:35,094 - INFO - Processing page 2\n",
      "2025-03-26 18:47:36,139 - INFO - Saved reviews data to ../data/raw/raw_reviews_1-2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/raw/raw_reviews_1-2.csv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "extract_reviews(company_name=COMPANY_NAME, start_page=1, end_page=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data collection\n",
    "There are over 3,000 review pages for Back Market, making it time-consuming to scrape all the data. To avoid overloading the server, we ran the extraction function in batches of 25 pages, incorporating significant delays between batches. The initial scraped data has been saved in the \"data/raw\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all extracted reviews into one csv file\n",
    "def concatenate_reviews(input_dir=\"../data/raw\", \\\n",
    "                        output_file=\"raw_reviews_0.csv\", \\\n",
    "                        file_prefix=\"raw_reviews_\"):\n",
    "    \"\"\"\n",
    "    Concatenates multiple review CSV files into a single DataFrame,\n",
    "    removes duplicates and missing values, \n",
    "    saves the cleaned data to a new CSV file,\n",
    "    and moves processed files to an '.archive' folder.\n",
    "    \"\"\"\n",
    "    # Validate input directory\n",
    "    if not os.path.exists(input_dir):\n",
    "        logging.error(f\"Input directory '{input_dir}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # List files matching the prefix\n",
    "    files = [f for f in os.listdir(input_dir) if f.startswith(file_prefix)]\n",
    "    if not files:\n",
    "        logging.error(f\"No files found with prefix '{file_prefix}' in '{input_dir}'.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Found {len(files)} files to process.\")\n",
    "\n",
    "    # Create .archive folder if it doesn't exist\n",
    "    archive_dir = os.path.join(input_dir, \".archive\")\n",
    "    os.makedirs(archive_dir, exist_ok=True)\n",
    "    logging.info(f\"Created/verified '.archive' folder at: {archive_dir}\")\n",
    "\n",
    "    # Read and concatenate files efficiently\n",
    "    try:\n",
    "        df_list = []\n",
    "        for f in files:\n",
    "            file_path = os.path.join(input_dir, f)\n",
    "            logging.info(f\"Reading file: {file_path}\")\n",
    "            df_list.append(pd.read_csv(file_path))\n",
    "        \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Concatenated all files into a single DataFrame.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading or concatenating files: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Log initial state\n",
    "    logging.info(f\"Initial DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "    logging.info(f\"Removed duplicates. New DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Drop rows with missing values in critical columns\n",
    "    critical_columns = [\"id\", \"review\", \"rating\", \"experienceDate\"]\n",
    "    df.dropna(subset=critical_columns, inplace=True)\n",
    "    logging.info(f\"Removed rows with missing values. Final DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Save cleaned DataFrame to CSV\n",
    "    try:\n",
    "        output_path = os.path.join(input_dir, output_file)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Saved cleaned DataFrame to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cleaned DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Move processed files to .archive folder\n",
    "    try:\n",
    "        for f in files:\n",
    "            src_path = os.path.join(input_dir, f)\n",
    "            dst_path = os.path.join(archive_dir, f)\n",
    "            shutil.move(src_path, dst_path)\n",
    "            logging.info(f\"Moved file to archive: {dst_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error moving files to archive: {e}\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:51:53,962 - INFO - Found 102 files to process.\n",
      "2025-03-26 18:51:53,963 - INFO - Created/verified '.archive' folder at: ../data/raw/.archive\n",
      "2025-03-26 18:51:53,965 - INFO - Reading file: ../data/raw/raw_reviews_850-875.csv\n",
      "2025-03-26 18:51:53,973 - INFO - Reading file: ../data/raw/raw_reviews_2025-2050.csv\n",
      "2025-03-26 18:51:53,978 - INFO - Reading file: ../data/raw/raw_reviews_2000-2025.csv\n",
      "2025-03-26 18:51:53,981 - INFO - Reading file: ../data/raw/raw_reviews_2700-2725.csv\n",
      "2025-03-26 18:51:53,987 - INFO - Reading file: ../data/raw/raw_reviews_2725-2750.csv\n",
      "2025-03-26 18:51:53,991 - INFO - Reading file: ../data/raw/raw_reviews_2900-2925.csv\n",
      "2025-03-26 18:51:53,996 - INFO - Reading file: ../data/raw/raw_reviews_950-975.csv\n",
      "2025-03-26 18:51:53,998 - INFO - Reading file: ../data/raw/raw_reviews_2075-2100.csv\n",
      "2025-03-26 18:51:54,002 - INFO - Reading file: ../data/raw/raw_reviews_2250-2275.csv\n",
      "2025-03-26 18:51:54,005 - INFO - Reading file: ../data/raw/raw_reviews_2925-2950.csv\n",
      "2025-03-26 18:51:54,009 - INFO - Reading file: ../data/raw/raw_reviews_2550-2575.csv\n",
      "2025-03-26 18:51:54,012 - INFO - Reading file: ../data/raw/raw_reviews_1125-1150.csv\n",
      "2025-03-26 18:51:54,016 - INFO - Reading file: ../data/raw/raw_reviews_3025-3050.csv\n",
      "2025-03-26 18:51:54,019 - INFO - Reading file: ../data/raw/raw_reviews_1100-1125.csv\n",
      "2025-03-26 18:51:54,022 - INFO - Reading file: ../data/raw/raw_reviews_1875-1900.csv\n",
      "2025-03-26 18:51:54,024 - INFO - Reading file: ../data/raw/raw_reviews_3000-3025.csv\n",
      "2025-03-26 18:51:54,028 - INFO - Reading file: ../data/raw/raw_reviews_1600-1625.csv\n",
      "2025-03-26 18:51:54,032 - INFO - Reading file: ../data/raw/raw_reviews_1625-1650.csv\n",
      "2025-03-26 18:51:54,034 - INFO - Reading file: ../data/raw/raw_reviews_1175-1200.csv\n",
      "2025-03-26 18:51:54,037 - INFO - Reading file: ../data/raw/raw_reviews_750-775.csv\n",
      "2025-03-26 18:51:54,039 - INFO - Reading file: ../data/raw/raw_reviews_1775-1800.csv\n",
      "2025-03-26 18:51:54,042 - INFO - Reading file: ../data/raw/raw_reviews_1800-1825.csv\n",
      "2025-03-26 18:51:54,044 - INFO - Reading file: ../data/raw/raw_reviews_3075-3100.csv\n",
      "2025-03-26 18:51:54,047 - INFO - Reading file: ../data/raw/raw_reviews_1350-1375.csv\n",
      "2025-03-26 18:51:54,049 - INFO - Reading file: ../data/raw/raw_reviews_1825-1850.csv\n",
      "2025-03-26 18:51:54,052 - INFO - Reading file: ../data/raw/raw_reviews_1450-1475.csv\n",
      "2025-03-26 18:51:54,054 - INFO - Reading file: ../data/raw/raw_reviews_1675-1700.csv\n",
      "2025-03-26 18:51:54,057 - INFO - Reading file: ../data/raw/raw_reviews_2050-2075.csv\n",
      "2025-03-26 18:51:54,066 - INFO - Reading file: ../data/raw/raw_reviews_875-900.csv\n",
      "2025-03-26 18:51:54,078 - INFO - Reading file: ../data/raw/raw_reviews_1375-1400.csv\n",
      "2025-03-26 18:51:54,083 - INFO - Reading file: ../data/raw/raw_reviews_2275-2300.csv\n",
      "2025-03-26 18:51:54,090 - INFO - Reading file: ../data/raw/raw_reviews_725-750.csv\n",
      "2025-03-26 18:51:54,093 - INFO - Reading file: ../data/raw/raw_reviews_2750-2775.csv\n",
      "2025-03-26 18:51:54,096 - INFO - Reading file: ../data/raw/raw_reviews_2200-2225.csv\n",
      "2025-03-26 18:51:54,099 - INFO - Reading file: ../data/raw/raw_reviews_2225-2250.csv\n",
      "2025-03-26 18:51:54,101 - INFO - Reading file: ../data/raw/raw_reviews_2575-2600.csv\n",
      "2025-03-26 18:51:54,103 - INFO - Reading file: ../data/raw/raw_reviews_2950-2975.csv\n",
      "2025-03-26 18:51:54,106 - INFO - Reading file: ../data/raw/raw_reviews_2975-3000.csv\n",
      "2025-03-26 18:51:54,109 - INFO - Reading file: ../data/raw/raw_reviews_2525-2550.csv\n",
      "2025-03-26 18:51:54,112 - INFO - Reading file: ../data/raw/raw_reviews_700-725.csv\n",
      "2025-03-26 18:51:54,115 - INFO - Reading file: ../data/raw/raw_reviews_2500-2525.csv\n",
      "2025-03-26 18:51:54,117 - INFO - Reading file: ../data/raw/raw_reviews_3050-3075.csv\n",
      "2025-03-26 18:51:54,121 - INFO - Reading file: ../data/raw/raw_reviews_900-925.csv\n",
      "2025-03-26 18:51:54,124 - INFO - Reading file: ../data/raw/raw_reviews_1150-1175.csv\n",
      "2025-03-26 18:51:54,126 - INFO - Reading file: ../data/raw/raw_reviews_825-850.csv\n",
      "2025-03-26 18:51:54,128 - INFO - Reading file: ../data/raw/raw_reviews_1475-1500.csv\n",
      "2025-03-26 18:51:54,132 - INFO - Reading file: ../data/raw/raw_reviews_1650-1675.csv\n",
      "2025-03-26 18:51:54,134 - INFO - Reading file: ../data/raw/raw_reviews_1300-1325.csv\n",
      "2025-03-26 18:51:54,137 - INFO - Reading file: ../data/raw/raw_reviews_800-825.csv\n",
      "2025-03-26 18:51:54,140 - INFO - Reading file: ../data/raw/raw_reviews_3200-3225.csv\n",
      "2025-03-26 18:51:54,143 - INFO - Reading file: ../data/raw/raw_reviews_925-950.csv\n",
      "2025-03-26 18:51:54,146 - INFO - Reading file: ../data/raw/raw_reviews_1325-1350.csv\n",
      "2025-03-26 18:51:54,148 - INFO - Reading file: ../data/raw/raw_reviews_1850-1875.csv\n",
      "2025-03-26 18:51:54,151 - INFO - Reading file: ../data/raw/raw_reviews_3225-3250.csv\n",
      "2025-03-26 18:51:54,153 - INFO - Reading file: ../data/raw/raw_reviews_1425-1450.csv\n",
      "2025-03-26 18:51:54,156 - INFO - Reading file: ../data/raw/raw_reviews_1400-1425.csv\n",
      "2025-03-26 18:51:54,158 - INFO - Reading file: ../data/raw/raw_reviews_1200-1225.csv\n",
      "2025-03-26 18:51:54,161 - INFO - Reading file: ../data/raw/raw_reviews_1225-1250.csv\n",
      "2025-03-26 18:51:54,164 - INFO - Reading file: ../data/raw/raw_reviews_1575-1600.csv\n",
      "2025-03-26 18:51:54,167 - INFO - Reading file: ../data/raw/raw_reviews_1950-1975.csv\n",
      "2025-03-26 18:51:54,169 - INFO - Reading file: ../data/raw/raw_reviews_975-1000.csv\n",
      "2025-03-26 18:51:54,171 - INFO - Reading file: ../data/raw/raw_reviews_1525-1550.csv\n",
      "2025-03-26 18:51:54,174 - INFO - Reading file: ../data/raw/raw_reviews_775-800.csv\n",
      "2025-03-26 18:51:54,177 - INFO - Reading file: ../data/raw/raw_reviews_1500-1525.csv\n",
      "2025-03-26 18:51:54,180 - INFO - Reading file: ../data/raw/raw_reviews_1050-1075.csv\n",
      "2025-03-26 18:51:54,183 - INFO - Reading file: ../data/raw/raw_reviews_3150-3175.csv\n",
      "2025-03-26 18:51:54,185 - INFO - Reading file: ../data/raw/raw_reviews_2375-2400.csv\n",
      "2025-03-26 18:51:54,188 - INFO - Reading file: ../data/raw/raw_reviews_1275-1300.csv\n",
      "2025-03-26 18:51:54,190 - INFO - Reading file: ../data/raw/raw_reviews_1750-1775.csv\n",
      "2025-03-26 18:51:54,192 - INFO - Reading file: ../data/raw/raw_reviews_2300-2325.csv\n",
      "2025-03-26 18:51:54,195 - INFO - Reading file: ../data/raw/raw_reviews_2325-2350.csv\n",
      "2025-03-26 18:51:54,199 - INFO - Reading file: ../data/raw/raw_reviews_2850-2875.csv\n",
      "2025-03-26 18:51:54,202 - INFO - Reading file: ../data/raw/raw_reviews_2425-2450.csv\n",
      "2025-03-26 18:51:54,205 - INFO - Reading file: ../data/raw/raw_reviews_2400-2425.csv\n",
      "2025-03-26 18:51:54,207 - INFO - Reading file: ../data/raw/raw_reviews_2150-2175.csv\n",
      "2025-03-26 18:51:54,209 - INFO - Reading file: ../data/raw/raw_reviews_2475-2500.csv\n",
      "2025-03-26 18:51:54,212 - INFO - Reading file: ../data/raw/raw_reviews_2650-2675.csv\n",
      "2025-03-26 18:51:54,215 - INFO - Reading file: ../data/raw/raw_reviews_1975-2000.csv\n",
      "2025-03-26 18:51:54,217 - INFO - Reading file: ../data/raw/raw_reviews_1900-1925.csv\n",
      "2025-03-26 18:51:54,220 - INFO - Reading file: ../data/raw/raw_reviews_1075-1100.csv\n",
      "2025-03-26 18:51:54,222 - INFO - Reading file: ../data/raw/raw_reviews_1250-1275.csv\n",
      "2025-03-26 18:51:54,226 - INFO - Reading file: ../data/raw/raw_reviews_1925-1950.csv\n",
      "2025-03-26 18:51:54,228 - INFO - Reading file: ../data/raw/raw_reviews_1550-1575.csv\n",
      "2025-03-26 18:51:54,231 - INFO - Reading file: ../data/raw/raw_reviews_3125-3150.csv\n",
      "2025-03-26 18:51:54,234 - INFO - Reading file: ../data/raw/raw_reviews_1025-1050.csv\n",
      "2025-03-26 18:51:54,236 - INFO - Reading file: ../data/raw/raw_reviews_3100-3125.csv\n",
      "2025-03-26 18:51:54,238 - INFO - Reading file: ../data/raw/raw_reviews_1000-1025.csv\n",
      "2025-03-26 18:51:54,241 - INFO - Reading file: ../data/raw/raw_reviews_1700-1725.csv\n",
      "2025-03-26 18:51:54,243 - INFO - Reading file: ../data/raw/raw_reviews_3175-3200.csv\n",
      "2025-03-26 18:51:54,247 - INFO - Reading file: ../data/raw/raw_reviews_1725-1750.csv\n",
      "2025-03-26 18:51:54,249 - INFO - Reading file: ../data/raw/raw_reviews_2800-2825.csv\n",
      "2025-03-26 18:51:54,252 - INFO - Reading file: ../data/raw/raw_reviews_2350-2375.csv\n",
      "2025-03-26 18:51:54,254 - INFO - Reading file: ../data/raw/raw_reviews_2825-2850.csv\n",
      "2025-03-26 18:51:54,256 - INFO - Reading file: ../data/raw/raw_reviews_2450-2475.csv\n",
      "2025-03-26 18:51:54,258 - INFO - Reading file: ../data/raw/raw_reviews_2675-2700.csv\n",
      "2025-03-26 18:51:54,261 - INFO - Reading file: ../data/raw/raw_reviews_2125-2150.csv\n",
      "2025-03-26 18:51:54,264 - INFO - Reading file: ../data/raw/raw_reviews_2100-2125.csv\n",
      "2025-03-26 18:51:54,266 - INFO - Reading file: ../data/raw/raw_reviews_2875-2900.csv\n",
      "2025-03-26 18:51:54,269 - INFO - Reading file: ../data/raw/raw_reviews_2600-2625.csv\n",
      "2025-03-26 18:51:54,271 - INFO - Reading file: ../data/raw/raw_reviews_2625-2650.csv\n",
      "2025-03-26 18:51:54,274 - INFO - Reading file: ../data/raw/raw_reviews_2175-2200.csv\n",
      "2025-03-26 18:51:54,276 - INFO - Reading file: ../data/raw/raw_reviews_2775-2800.csv\n",
      "2025-03-26 18:51:54,284 - INFO - Concatenated all files into a single DataFrame.\n",
      "2025-03-26 18:51:54,291 - INFO - Initial DataFrame info:\n",
      "None\n",
      "2025-03-26 18:51:54,303 - INFO - Removed duplicates. New DataFrame info:\n",
      "None\n",
      "2025-03-26 18:51:54,316 - INFO - Removed rows with missing values. Final DataFrame info:\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52960 entries, 0 to 52959\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               52960 non-null  object\n",
      " 1   title            52960 non-null  object\n",
      " 2   review           52960 non-null  object\n",
      " 3   rating           52960 non-null  int64 \n",
      " 4   reply            8311 non-null   object\n",
      " 5   experienceDate   52467 non-null  object\n",
      " 6   createdDateTime  52960 non-null  object\n",
      " 7   publishedDate    52960 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50972 entries, 0 to 52939\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               50972 non-null  object\n",
      " 1   title            50972 non-null  object\n",
      " 2   review           50972 non-null  object\n",
      " 3   rating           50972 non-null  int64 \n",
      " 4   reply            8012 non-null   object\n",
      " 5   experienceDate   50498 non-null  object\n",
      " 6   createdDateTime  50972 non-null  object\n",
      " 7   publishedDate    50972 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50498 entries, 0 to 52939\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               50498 non-null  object\n",
      " 1   title            50498 non-null  object\n",
      " 2   review           50498 non-null  object\n",
      " 3   rating           50498 non-null  int64 \n",
      " 4   reply            7838 non-null   object\n",
      " 5   experienceDate   50498 non-null  object\n",
      " 6   createdDateTime  50498 non-null  object\n",
      " 7   publishedDate    50498 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.5+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:51:54,515 - INFO - Saved cleaned DataFrame to: ../data/raw/raw_reviews_0.csv\n",
      "2025-03-26 18:51:54,516 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_850-875.csv\n",
      "2025-03-26 18:51:54,516 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2025-2050.csv\n",
      "2025-03-26 18:51:54,516 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2000-2025.csv\n",
      "2025-03-26 18:51:54,516 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2700-2725.csv\n",
      "2025-03-26 18:51:54,517 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2725-2750.csv\n",
      "2025-03-26 18:51:54,517 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2900-2925.csv\n",
      "2025-03-26 18:51:54,517 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_950-975.csv\n",
      "2025-03-26 18:51:54,517 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2075-2100.csv\n",
      "2025-03-26 18:51:54,518 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2250-2275.csv\n",
      "2025-03-26 18:51:54,518 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2925-2950.csv\n",
      "2025-03-26 18:51:54,518 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2550-2575.csv\n",
      "2025-03-26 18:51:54,518 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1125-1150.csv\n",
      "2025-03-26 18:51:54,519 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3025-3050.csv\n",
      "2025-03-26 18:51:54,519 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1100-1125.csv\n",
      "2025-03-26 18:51:54,519 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1875-1900.csv\n",
      "2025-03-26 18:51:54,519 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3000-3025.csv\n",
      "2025-03-26 18:51:54,520 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1600-1625.csv\n",
      "2025-03-26 18:51:54,520 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1625-1650.csv\n",
      "2025-03-26 18:51:54,520 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1175-1200.csv\n",
      "2025-03-26 18:51:54,520 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_750-775.csv\n",
      "2025-03-26 18:51:54,521 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1775-1800.csv\n",
      "2025-03-26 18:51:54,521 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1800-1825.csv\n",
      "2025-03-26 18:51:54,521 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3075-3100.csv\n",
      "2025-03-26 18:51:54,521 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1350-1375.csv\n",
      "2025-03-26 18:51:54,522 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1825-1850.csv\n",
      "2025-03-26 18:51:54,522 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1450-1475.csv\n",
      "2025-03-26 18:51:54,522 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1675-1700.csv\n",
      "2025-03-26 18:51:54,522 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2050-2075.csv\n",
      "2025-03-26 18:51:54,523 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_875-900.csv\n",
      "2025-03-26 18:51:54,523 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1375-1400.csv\n",
      "2025-03-26 18:51:54,523 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2275-2300.csv\n",
      "2025-03-26 18:51:54,523 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_725-750.csv\n",
      "2025-03-26 18:51:54,524 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2750-2775.csv\n",
      "2025-03-26 18:51:54,524 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2200-2225.csv\n",
      "2025-03-26 18:51:54,524 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2225-2250.csv\n",
      "2025-03-26 18:51:54,524 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2575-2600.csv\n",
      "2025-03-26 18:51:54,525 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2950-2975.csv\n",
      "2025-03-26 18:51:54,525 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2975-3000.csv\n",
      "2025-03-26 18:51:54,525 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2525-2550.csv\n",
      "2025-03-26 18:51:54,525 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_700-725.csv\n",
      "2025-03-26 18:51:54,526 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2500-2525.csv\n",
      "2025-03-26 18:51:54,526 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3050-3075.csv\n",
      "2025-03-26 18:51:54,526 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_900-925.csv\n",
      "2025-03-26 18:51:54,526 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1150-1175.csv\n",
      "2025-03-26 18:51:54,526 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_825-850.csv\n",
      "2025-03-26 18:51:54,527 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1475-1500.csv\n",
      "2025-03-26 18:51:54,527 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1650-1675.csv\n",
      "2025-03-26 18:51:54,528 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1300-1325.csv\n",
      "2025-03-26 18:51:54,528 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_800-825.csv\n",
      "2025-03-26 18:51:54,533 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3200-3225.csv\n",
      "2025-03-26 18:51:54,534 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_925-950.csv\n",
      "2025-03-26 18:51:54,535 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1325-1350.csv\n",
      "2025-03-26 18:51:54,535 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1850-1875.csv\n",
      "2025-03-26 18:51:54,536 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3225-3250.csv\n",
      "2025-03-26 18:51:54,537 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1425-1450.csv\n",
      "2025-03-26 18:51:54,537 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1400-1425.csv\n",
      "2025-03-26 18:51:54,537 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1200-1225.csv\n",
      "2025-03-26 18:51:54,537 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1225-1250.csv\n",
      "2025-03-26 18:51:54,538 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1575-1600.csv\n",
      "2025-03-26 18:51:54,538 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1950-1975.csv\n",
      "2025-03-26 18:51:54,538 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_975-1000.csv\n",
      "2025-03-26 18:51:54,538 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1525-1550.csv\n",
      "2025-03-26 18:51:54,539 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_775-800.csv\n",
      "2025-03-26 18:51:54,539 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1500-1525.csv\n",
      "2025-03-26 18:51:54,539 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1050-1075.csv\n",
      "2025-03-26 18:51:54,539 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3150-3175.csv\n",
      "2025-03-26 18:51:54,540 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2375-2400.csv\n",
      "2025-03-26 18:51:54,540 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1275-1300.csv\n",
      "2025-03-26 18:51:54,540 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1750-1775.csv\n",
      "2025-03-26 18:51:54,540 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2300-2325.csv\n",
      "2025-03-26 18:51:54,541 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2325-2350.csv\n",
      "2025-03-26 18:51:54,541 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2850-2875.csv\n",
      "2025-03-26 18:51:54,541 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2425-2450.csv\n",
      "2025-03-26 18:51:54,541 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2400-2425.csv\n",
      "2025-03-26 18:51:54,541 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2150-2175.csv\n",
      "2025-03-26 18:51:54,542 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2475-2500.csv\n",
      "2025-03-26 18:51:54,542 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2650-2675.csv\n",
      "2025-03-26 18:51:54,542 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1975-2000.csv\n",
      "2025-03-26 18:51:54,542 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1900-1925.csv\n",
      "2025-03-26 18:51:54,543 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1075-1100.csv\n",
      "2025-03-26 18:51:54,543 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1250-1275.csv\n",
      "2025-03-26 18:51:54,543 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1925-1950.csv\n",
      "2025-03-26 18:51:54,543 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1550-1575.csv\n",
      "2025-03-26 18:51:54,544 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3125-3150.csv\n",
      "2025-03-26 18:51:54,544 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1025-1050.csv\n",
      "2025-03-26 18:51:54,544 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3100-3125.csv\n",
      "2025-03-26 18:51:54,544 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1000-1025.csv\n",
      "2025-03-26 18:51:54,545 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1700-1725.csv\n",
      "2025-03-26 18:51:54,545 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_3175-3200.csv\n",
      "2025-03-26 18:51:54,545 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1725-1750.csv\n",
      "2025-03-26 18:51:54,546 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2800-2825.csv\n",
      "2025-03-26 18:51:54,546 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2350-2375.csv\n",
      "2025-03-26 18:51:54,546 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2825-2850.csv\n",
      "2025-03-26 18:51:54,546 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2450-2475.csv\n",
      "2025-03-26 18:51:54,547 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2675-2700.csv\n",
      "2025-03-26 18:51:54,547 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2125-2150.csv\n",
      "2025-03-26 18:51:54,547 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2100-2125.csv\n",
      "2025-03-26 18:51:54,547 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2875-2900.csv\n",
      "2025-03-26 18:51:54,548 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2600-2625.csv\n",
      "2025-03-26 18:51:54,548 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2625-2650.csv\n",
      "2025-03-26 18:51:54,548 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2175-2200.csv\n",
      "2025-03-26 18:51:54,548 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2775-2800.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50498 entries, 0 to 52939\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               50498 non-null  object\n",
      " 1   title            50498 non-null  object\n",
      " 2   review           50498 non-null  object\n",
      " 3   rating           50498 non-null  int64 \n",
      " 4   reply            7838 non-null   object\n",
      " 5   experienceDate   50498 non-null  object\n",
      " 6   createdDateTime  50498 non-null  object\n",
      " 7   publishedDate    50498 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>reply</th>\n",
       "      <th>experienceDate</th>\n",
       "      <th>createdDateTime</th>\n",
       "      <th>publishedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640053299b64b1bdaf6661e9</td>\n",
       "      <td>Satisfait de mon choix avec backmarket</td>\n",
       "      <td>Satisfait de mon choix avec backmarket</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T09:41:29.000Z</td>\n",
       "      <td>2023-03-02T09:41:29.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640046cda2e3a177e9e8b614</td>\n",
       "      <td>A fuir, aucun service client</td>\n",
       "      <td>A éviter ! Pas reçu le bon article, je demande...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01T00:00:00.000Z</td>\n",
       "      <td>2023-03-02T08:48:45.000Z</td>\n",
       "      <td>2023-03-02T08:48:45.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6400387bd143b326fe43200d</td>\n",
       "      <td>Parfait,rapide,bien emballé</td>\n",
       "      <td>Parfait,rapide,bien emballé</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T07:47:39.000Z</td>\n",
       "      <td>2023-03-02T07:47:39.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>640034a7d143b326fe431f53</td>\n",
       "      <td>Pentax</td>\n",
       "      <td>Livraison très rapide</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T07:31:19.000Z</td>\n",
       "      <td>2023-03-02T07:31:19.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>640030139b64b1bdaf665737</td>\n",
       "      <td>Mécontent car produit endommagéBien que le col...</td>\n",
       "      <td>Bien que le colis soit arrivé tôt j'ai globale...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-27T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T07:11:47.000Z</td>\n",
       "      <td>2023-03-02T07:11:47.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  640053299b64b1bdaf6661e9   \n",
       "1  640046cda2e3a177e9e8b614   \n",
       "2  6400387bd143b326fe43200d   \n",
       "3  640034a7d143b326fe431f53   \n",
       "4  640030139b64b1bdaf665737   \n",
       "\n",
       "                                               title  \\\n",
       "0             Satisfait de mon choix avec backmarket   \n",
       "1                       A fuir, aucun service client   \n",
       "2                        Parfait,rapide,bien emballé   \n",
       "3                                             Pentax   \n",
       "4  Mécontent car produit endommagéBien que le col...   \n",
       "\n",
       "                                              review  rating reply  \\\n",
       "0             Satisfait de mon choix avec backmarket       5   NaN   \n",
       "1  A éviter ! Pas reçu le bon article, je demande...       1   NaN   \n",
       "2                        Parfait,rapide,bien emballé       5   NaN   \n",
       "3                              Livraison très rapide       5   NaN   \n",
       "4  Bien que le colis soit arrivé tôt j'ai globale...       1   NaN   \n",
       "\n",
       "             experienceDate           createdDateTime  \\\n",
       "0  2023-02-28T23:00:00.000Z  2023-03-02T09:41:29.000Z   \n",
       "1  2023-03-01T00:00:00.000Z  2023-03-02T08:48:45.000Z   \n",
       "2  2023-02-28T23:00:00.000Z  2023-03-02T07:47:39.000Z   \n",
       "3  2023-02-28T23:00:00.000Z  2023-03-02T07:31:19.000Z   \n",
       "4  2023-02-27T23:00:00.000Z  2023-03-02T07:11:47.000Z   \n",
       "\n",
       "              publishedDate  \n",
       "0  2023-03-02T09:41:29.000Z  \n",
       "1  2023-03-02T08:48:45.000Z  \n",
       "2  2023-03-02T07:47:39.000Z  \n",
       "3  2023-03-02T07:31:19.000Z  \n",
       "4  2023-03-02T07:11:47.000Z  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all extracted reviews into one csv file\n",
    "df = concatenate_reviews()\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51018 entries, 0 to 51017\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               51018 non-null  object\n",
      " 1   title            51018 non-null  object\n",
      " 2   review           51018 non-null  object\n",
      " 3   rating           51018 non-null  int64 \n",
      " 4   reply            7838 non-null   object\n",
      " 5   experienceDate   51018 non-null  object\n",
      " 6   createdDateTime  51018 non-null  object\n",
      " 7   publishedDate    51018 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 3.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>reply</th>\n",
       "      <th>experienceDate</th>\n",
       "      <th>createdDateTime</th>\n",
       "      <th>publishedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640053299b64b1bdaf6661e9</td>\n",
       "      <td>Satisfait de mon choix avec backmarket</td>\n",
       "      <td>Satisfait de mon choix avec backmarket</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T09:41:29.000Z</td>\n",
       "      <td>2023-03-02T09:41:29.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640046cda2e3a177e9e8b614</td>\n",
       "      <td>A fuir, aucun service client</td>\n",
       "      <td>A éviter ! Pas reçu le bon article, je demande...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01T00:00:00.000Z</td>\n",
       "      <td>2023-03-02T08:48:45.000Z</td>\n",
       "      <td>2023-03-02T08:48:45.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6400387bd143b326fe43200d</td>\n",
       "      <td>Parfait,rapide,bien emballé</td>\n",
       "      <td>Parfait,rapide,bien emballé</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T07:47:39.000Z</td>\n",
       "      <td>2023-03-02T07:47:39.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>640034a7d143b326fe431f53</td>\n",
       "      <td>Pentax</td>\n",
       "      <td>Livraison très rapide</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T07:31:19.000Z</td>\n",
       "      <td>2023-03-02T07:31:19.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>640030139b64b1bdaf665737</td>\n",
       "      <td>Mécontent car produit endommagéBien que le col...</td>\n",
       "      <td>Bien que le colis soit arrivé tôt j'ai globale...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-27T23:00:00.000Z</td>\n",
       "      <td>2023-03-02T07:11:47.000Z</td>\n",
       "      <td>2023-03-02T07:11:47.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  640053299b64b1bdaf6661e9   \n",
       "1  640046cda2e3a177e9e8b614   \n",
       "2  6400387bd143b326fe43200d   \n",
       "3  640034a7d143b326fe431f53   \n",
       "4  640030139b64b1bdaf665737   \n",
       "\n",
       "                                               title  \\\n",
       "0             Satisfait de mon choix avec backmarket   \n",
       "1                       A fuir, aucun service client   \n",
       "2                        Parfait,rapide,bien emballé   \n",
       "3                                             Pentax   \n",
       "4  Mécontent car produit endommagéBien que le col...   \n",
       "\n",
       "                                              review  rating reply  \\\n",
       "0             Satisfait de mon choix avec backmarket       5   NaN   \n",
       "1  A éviter ! Pas reçu le bon article, je demande...       1   NaN   \n",
       "2                        Parfait,rapide,bien emballé       5   NaN   \n",
       "3                              Livraison très rapide       5   NaN   \n",
       "4  Bien que le colis soit arrivé tôt j'ai globale...       1   NaN   \n",
       "\n",
       "             experienceDate           createdDateTime  \\\n",
       "0  2023-02-28T23:00:00.000Z  2023-03-02T09:41:29.000Z   \n",
       "1  2023-03-01T00:00:00.000Z  2023-03-02T08:48:45.000Z   \n",
       "2  2023-02-28T23:00:00.000Z  2023-03-02T07:47:39.000Z   \n",
       "3  2023-02-28T23:00:00.000Z  2023-03-02T07:31:19.000Z   \n",
       "4  2023-02-27T23:00:00.000Z  2023-03-02T07:11:47.000Z   \n",
       "\n",
       "              publishedDate  \n",
       "0  2023-03-02T09:41:29.000Z  \n",
       "1  2023-03-02T08:48:45.000Z  \n",
       "2  2023-03-02T07:47:39.000Z  \n",
       "3  2023-03-02T07:31:19.000Z  \n",
       "4  2023-03-02T07:11:47.000Z  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process reviews data\n",
    "\n",
    "In this step we preprocess raw text data for Natural Language Processing (NLP) tasks by performing a series of cleaning and normalization steps. We removes noise (e.g., hashtags, URLs, mentions, stopwords), converts text to lowercase, tokenizes it, filters out non-alphabetic tokens, and returns the cleaned text as a single string.\n",
    "\n",
    "#### Text cleaning and normalization\n",
    "- Noise Removal :\n",
    "    - Removes hashtags, HTML entities, stock tickers, URLs, retweet tags, mentions, and special characters.\n",
    "    - Replaces emojis with their textual descriptions and strips punctuation.\n",
    "\n",
    "- Normalization :\n",
    "    - Converts text to lowercase, replaces ampersands (&) with \"and,\" and removes short words (≤3 characters).\n",
    "\n",
    "- Tokenization :\n",
    "    - Splits text into tokens using French-specific word tokenization.\n",
    "\n",
    "- Filtering :\n",
    "    - Removes numbers, non-alphabetic tokens, and stopwords (including custom additions to the French stopwords list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love this product great fast service\n"
     ]
    }
   ],
   "source": [
    "# Function to clean and process review text\n",
    "\n",
    "# os.environ[\"NLTK_DATA\"] = \"/Users/micheldpd/Projects/custrev/nltk_data\"\n",
    "\n",
    "# Preload stopwords outside the function to avoid repeated loading\n",
    "STOP_WORDS_TO_ADD = [\"être\", \"leur\", \"leurs\", \"avoir\", \"cela\", \"les\", \"de\", \"pour\", \"des\", \"cette\", \"a\",\n",
    "                   \"j'ai\", \"car\", \"c'est\", \"chez\", \"tout\", \"fait\", \"chez\", \"donc\", \n",
    "                   \"n'est\", \"si\", \"alors\", \"n'ai\", \"faire\", \"deux\", \"comme\", \"jour\", \"tr\", \"si\", \"ue\"\n",
    "\n",
    "]\n",
    "STOP_WORDS = set(stopwords.words('french')).union(set(STOP_WORDS_TO_ADD))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing noise (e.g., hashtags, URLs, stopwords) and normalizing content.\n",
    "    Tokenizes, filters alphabetic tokens, and removes French stopwords for NLP tasks.\n",
    "    Returns the cleaned and normalized text as a single string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove hashtags (keep text after #)\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Remove HTML special entities (e.g., &amp;)\n",
    "    text = re.sub(r'&\\w*;', '', text)\n",
    "\n",
    "    # Remove stock tickers (e.g., $AAPL)\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "\n",
    "    # Remove hyperlinks (covers various URL patterns)\n",
    "    text = re.sub(r'https?://[^\\s/$.?#].[^\\s]*', '', text)\n",
    "    text = re.sub(r'http(\\S)+', '', text)  # Catch incomplete URLs\n",
    "    text = re.sub(r'http\\s*\\.\\.\\.', '', text)  # Catch truncated URLs\n",
    "\n",
    "    # Remove retweet tags and mentions\n",
    "    text = re.sub(r'(RT|rt)\\s*@\\s*\\S+', '', text)\n",
    "    text = re.sub(r'RT\\s?@', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "    # Replace & with 'and', fix < and > (assuming intent was to escape them)\n",
    "    text = re.sub(r'&', 'and', text)\n",
    "\n",
    "    # Remove words with 3 or fewer letters (e.g., \"the\", \"cat\")\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', ' ', text)\n",
    "\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode\n",
    "    text = ''.join(c for c in text if ord(c) <= 0xFFFF)\n",
    "\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert emojis to text descriptions (e.g., 😊 -> :smiling_face:)\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove punctuation, keeping alphanumeric characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Tokenize text (lowercase for consistency)\n",
    "    tokens: List[str] = word_tokenize(text.lower(), language='french')\n",
    "\n",
    "    # Filter out numbers and keep only alphabetic tokens\n",
    "    tokens_alpha = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens_cleaned = [token for token in tokens_alpha if token not in STOP_WORDS]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens_cleaned)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"RT @user: I love this product! < and 16§789> #great https://example.com 😊 &amp; fast service\"\n",
    "    cleaned = clean_text(sample_text)\n",
    "    print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review data cleaning and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "def process_reviews(raw_file):\n",
    "    \"\"\"\n",
    "    Processes raw review data by standardizing dates, extracting temporal features, and cleaning text.\n",
    "    Removes rows with missing values, invalid ratings, or short reviews, ensuring data quality.\n",
    "    Saves the cleaned and filtered reviews to a timestamped CSV file for further analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load raw data\n",
    "        logging.info(f\"Loading raw data from: {raw_file}\")\n",
    "        df = pd.read_csv(raw_file)\n",
    "        logging.info(f\"Successfully loaded {len(df)} rows of data.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading raw data: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Standardize date formats\n",
    "        logging.info(\"Standardizing date formats...\")\n",
    "        df[\"experienceDate\"] = pd.to_datetime(df[\"experienceDate\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df[\"createdDateTime\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        logging.info(\"Date formats standardized successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error standardizing date formats: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract temporal features\n",
    "        logging.info(\"Extracting temporal features (year, month, day, hour)...\")\n",
    "        df['year'] = pd.to_datetime(df['createdDateTime']).dt.year\n",
    "        df[\"year_quarter\"] = pd.to_datetime(df['createdDateTime']).dt.year.astype(str) + \\\n",
    "            \"-Q\" + pd.to_datetime(df['createdDateTime']).dt.quarter.astype(str)\n",
    "        df['month'] = pd.to_datetime(df['createdDateTime']).dt.month\n",
    "        df['month_name'] = pd.to_datetime(df['createdDateTime']).dt.month_name()\n",
    "        df['day'] = pd.to_datetime(df['createdDateTime']).dt.day\n",
    "        df['day_name'] = pd.to_datetime(df['createdDateTime']).dt.day_name()\n",
    "        df['hour'] = pd.to_datetime(df['createdDateTime']).dt.hour\n",
    "        logging.info(\"Temporal features extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting temporal features: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Remove rows with missing values\n",
    "        logging.info(\"Removing rows with missing values...\")\n",
    "        initial_rows = len(df)\n",
    "        df.dropna(inplace=True, subset=[\"id\", \"review\", \"rating\", \"experienceDate\", \"createdDateTime\", \"publishedDate\"])\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} rows with missing values. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing rows with missing values: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Remove rows with invalid ratings\n",
    "        logging.info(\"Removing rows with invalid ratings...\")\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"rating\"].isin([1, 2, 3, 4, 5])]\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} rows with invalid ratings. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing rows with invalid ratings: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Clean review text\n",
    "        logging.info(\"Cleaning review text...\")\n",
    "        df[\"review\"] = df[\"review\"].apply(clean_text)\n",
    "        logging.info(\"Review text cleaned successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning review text: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Remove short reviews\n",
    "        logging.info(\"Removing short reviews...\")\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"review\"].str.len() > 4]\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} short reviews. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing short reviews: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Save cleaned data to CSV\n",
    "        logging.info(\"Saving cleaned data to CSV...\")\n",
    "        output_dir = \"../data/cleaned\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        cleaned_file_path = os.path.join(output_dir, f\"cleaned_reviews_{timestamp}.csv\")\n",
    "        df.to_csv(cleaned_file_path, index=False)\n",
    "        logging.info(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cleaned data: {e}\")\n",
    "        return None\n",
    "\n",
    "    return cleaned_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:52:42,983 - INFO - Loading raw data from: ../data/raw/raw_reviews_0.csv\n",
      "2025-03-26 18:52:43,151 - INFO - Successfully loaded 50498 rows of data.\n",
      "2025-03-26 18:52:43,151 - INFO - Standardizing date formats...\n",
      "2025-03-26 18:52:43,466 - INFO - Date formats standardized successfully.\n",
      "2025-03-26 18:52:43,467 - INFO - Extracting temporal features (year, month, day, hour)...\n",
      "2025-03-26 18:52:43,501 - INFO - Temporal features extracted successfully.\n",
      "2025-03-26 18:52:43,502 - INFO - Removing rows with missing values...\n",
      "2025-03-26 18:52:43,516 - INFO - Removed 0 rows with missing values. Remaining rows: 50498\n",
      "2025-03-26 18:52:43,517 - INFO - Removing rows with invalid ratings...\n",
      "2025-03-26 18:52:43,520 - INFO - Removed 0 rows with invalid ratings. Remaining rows: 50498\n",
      "2025-03-26 18:52:43,521 - INFO - Cleaning review text...\n",
      "2025-03-26 18:52:47,859 - INFO - Review text cleaned successfully.\n",
      "2025-03-26 18:52:47,860 - INFO - Removing short reviews...\n",
      "2025-03-26 18:52:47,869 - INFO - Removed 113 short reviews. Remaining rows: 50385\n",
      "2025-03-26 18:52:47,870 - INFO - Saving cleaned data to CSV...\n",
      "2025-03-26 18:52:48,056 - INFO - Cleaned data saved to: ../data/cleaned/cleaned_reviews_20250326185247.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/cleaned/cleaned_reviews_20250326185247.csv'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processed raw review\n",
    "process_reviews(\"../data/raw/raw_reviews_0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading cleaned review into the full review base\n",
    "\n",
    "After cleaning and processing, the extracted raw reviews are consolidated into the full review database.\n",
    "Before consolidation, the pipeline checks for the presence of new data. If no new data is found, the full review database remains unchanged, and the pipeline stops at this stage.\n",
    "Once the loading process is complete, the cleaned review file is archived to ensure proper organization and to maintain a history of processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(cleaned_file) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes review data by merging new data with existing cleaned data.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_file (str): Path to the new review data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame after merging and deduplication\n",
    "        None: If an error occurs during processing or if no new data is added\n",
    "        \n",
    "    Key Features:\n",
    "        - Ensures output directory exists\n",
    "        - Handles existing data loading with error checking\n",
    "        - Updates data only if new reviews are added\n",
    "        - Writes \"1\" to ../parameters/new_reviews.txt if new reviews exist, otherwise \"0\"\n",
    "        - Logs percentage increase in data when new reviews are added\n",
    "        - Saves updated data with timestamp backup only if necessary\n",
    "        - Archives the cleaned review file after processing\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load new review data\n",
    "        df = pd.read_csv(cleaned_file)\n",
    "\n",
    "        # Define paths\n",
    "        base_dir = \"../data\"\n",
    "        full_reviews_folder = os.path.join(base_dir, \"full\")\n",
    "        archive_folder = os.path.join(base_dir, \"archive\")  # Archive folder for processed files\n",
    "        full_reviews_file = \"full_reviews.csv\"\n",
    "        full_reviews_path = os.path.join(full_reviews_folder, full_reviews_file)\n",
    "        new_reviews_flag_path = os.path.join(\"../parameters\", \"new_reviews.txt\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(full_reviews_folder, exist_ok=True)\n",
    "        os.makedirs(archive_folder, exist_ok=True)  # Create archive folder\n",
    "        os.makedirs(os.path.dirname(new_reviews_flag_path), exist_ok=True)\n",
    "        logging.info(f\"Directories ensured: {full_reviews_folder}, {archive_folder}, {os.path.dirname(new_reviews_flag_path)}\")\n",
    "\n",
    "        # Load existing data with error handling\n",
    "        df_full = pd.DataFrame()\n",
    "        if os.path.isfile(full_reviews_path):\n",
    "            try:\n",
    "                df_full = pd.read_csv(full_reviews_path, low_memory=False)\n",
    "                logging.info(f\"Loaded existing data from {full_reviews_path}\")\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.warning(f\"Empty CSV file found at {full_reviews_path}\")\n",
    "            except pd.errors.ParserError:\n",
    "                logging.error(f\"Parsing error in {full_reviews_path}\")\n",
    "        else:\n",
    "            logging.info(f\"No existing file found at {full_reviews_path}, initializing empty DataFrame\")\n",
    "\n",
    "        # Validate input DataFrame\n",
    "        if df.empty:\n",
    "            logging.warning(\"Input DataFrame is empty\")\n",
    "            return None\n",
    "\n",
    "        # Merge and deduplicate\n",
    "        initial_length = len(df_full)\n",
    "        df_full_updated = pd.concat([df_full, df], ignore_index=True)\n",
    "        df_full_updated = df_full_updated.drop_duplicates(subset=df.columns, keep='last')\n",
    "        final_length = len(df_full_updated)\n",
    "\n",
    "        # Log DataFrame sizes\n",
    "        logging.info(f\"Original records: {initial_length}\")\n",
    "        logging.info(f\"Updated records: {final_length}\")\n",
    "        new_records_added = final_length - initial_length\n",
    "        logging.info(f\"New records added: {new_records_added}\")\n",
    "\n",
    "        # Check if there are new records\n",
    "        has_new_reviews = final_length > initial_length\n",
    "        flag_value = \"1\" if has_new_reviews else \"0\"\n",
    "        with open(new_reviews_flag_path, \"w\") as f:\n",
    "            f.write(flag_value)\n",
    "        logging.info(f\"Wrote '{flag_value}' to {new_reviews_flag_path}\")\n",
    "\n",
    "        # Exit early if no new reviews are added\n",
    "        if not has_new_reviews:\n",
    "            logging.info(\"No new reviews to process. Exiting without updates.\")\n",
    "            return df_full_updated\n",
    "\n",
    "        # Calculate percentage increase\n",
    "        if initial_length == 0 and final_length > 0:\n",
    "            percentage_increase = 100\n",
    "        elif initial_length > 0:\n",
    "            percentage_increase = ((final_length - initial_length) / initial_length * 100) \n",
    "        else: percentage_increase = 0\n",
    "        logging.info(f\"Percentage increase in data: {percentage_increase:.2f}%\")\n",
    "\n",
    "        # Create backup before overwriting\n",
    "        if os.path.isfile(full_reviews_path):\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_path = os.path.join(full_reviews_folder, f\"full_reviews_backup_{timestamp}.csv\")\n",
    "            df_full.to_csv(backup_path, index=False)\n",
    "            logging.info(f\"Backup created at {backup_path}\")\n",
    "\n",
    "        # Save updated data\n",
    "        df_full_updated.to_csv(full_reviews_path, index=False)\n",
    "        logging.info(f\"Updated data saved to {full_reviews_path}\")\n",
    "\n",
    "        # Archive the cleaned review file\n",
    "        try:\n",
    "            archive_path = os.path.join(archive_folder, os.path.basename(cleaned_file))\n",
    "            shutil.move(cleaned_file, archive_path)\n",
    "            logging.info(f\"Archived cleaned review file to: {archive_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error archiving cleaned review file: {e}\")\n",
    "\n",
    "        return df_full_updated\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing reviews: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:55:54,679 - INFO - Loading raw data from: ../data/raw/raw_reviews_0.csv\n",
      "2025-03-26 18:55:54,844 - INFO - Successfully loaded 50498 rows of data.\n",
      "2025-03-26 18:55:54,844 - INFO - Standardizing date formats...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " transformed reviews: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:55:55,188 - INFO - Date formats standardized successfully.\n",
      "2025-03-26 18:55:55,189 - INFO - Extracting temporal features (year, month, day, hour)...\n",
      "2025-03-26 18:55:55,223 - INFO - Temporal features extracted successfully.\n",
      "2025-03-26 18:55:55,224 - INFO - Removing rows with missing values...\n",
      "2025-03-26 18:55:55,238 - INFO - Removed 0 rows with missing values. Remaining rows: 50498\n",
      "2025-03-26 18:55:55,239 - INFO - Removing rows with invalid ratings...\n",
      "2025-03-26 18:55:55,243 - INFO - Removed 0 rows with invalid ratings. Remaining rows: 50498\n",
      "2025-03-26 18:55:55,244 - INFO - Cleaning review text...\n",
      "2025-03-26 18:55:59,564 - INFO - Review text cleaned successfully.\n",
      "2025-03-26 18:55:59,564 - INFO - Removing short reviews...\n",
      "2025-03-26 18:55:59,575 - INFO - Removed 113 short reviews. Remaining rows: 50385\n",
      "2025-03-26 18:55:59,575 - INFO - Saving cleaned data to CSV...\n",
      "2025-03-26 18:55:59,759 - INFO - Cleaned data saved to: ../data/cleaned/cleaned_reviews_20250326185559.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cleaned_file_path: ../data/cleaned/cleaned_reviews_20250326185559.csv \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50385 entries, 0 to 50384\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               50385 non-null  object\n",
      " 1   title            50385 non-null  object\n",
      " 2   review           50385 non-null  object\n",
      " 3   rating           50385 non-null  int64 \n",
      " 4   reply            7828 non-null   object\n",
      " 5   experienceDate   50385 non-null  object\n",
      " 6   createdDateTime  50385 non-null  object\n",
      " 7   publishedDate    50385 non-null  object\n",
      " 8   year             50385 non-null  int64 \n",
      " 9   month            50385 non-null  int64 \n",
      " 10  month_name       50385 non-null  object\n",
      " 11  day              50385 non-null  int64 \n",
      " 12  day_name         50385 non-null  object\n",
      " 13  hour             50385 non-null  int64 \n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 5.4+ MB\n",
      "None\n",
      "                         id  \\\n",
      "0  640053299b64b1bdaf6661e9   \n",
      "1  640046cda2e3a177e9e8b614   \n",
      "2  6400387bd143b326fe43200d   \n",
      "3  640034a7d143b326fe431f53   \n",
      "4  640030139b64b1bdaf665737   \n",
      "\n",
      "                                               title  \\\n",
      "0             Satisfait de mon choix avec backmarket   \n",
      "1                       A fuir, aucun service client   \n",
      "2                        Parfait,rapide,bien emballé   \n",
      "3                                             Pentax   \n",
      "4  Mécontent car produit endommagéBien que le col...   \n",
      "\n",
      "                                              review  rating reply  \\\n",
      "0                         satisfait choix backmarket       5   NaN   \n",
      "1  éviter reçu article demande retour fois avant ...       1   NaN   \n",
      "2                        parfait rapide bien emballé       5   NaN   \n",
      "3                              livraison très rapide       5   NaN   \n",
      "4  bien colis arrivé globalement décu batterie ar...       1   NaN   \n",
      "\n",
      "        experienceDate      createdDateTime        publishedDate  year  month  \\\n",
      "0  2023-02-28 23:00:00  2023-03-02 09:41:29  2023-03-02 09:41:29  2023      3   \n",
      "1  2023-03-01 00:00:00  2023-03-02 08:48:45  2023-03-02 08:48:45  2023      3   \n",
      "2  2023-02-28 23:00:00  2023-03-02 07:47:39  2023-03-02 07:47:39  2023      3   \n",
      "3  2023-02-28 23:00:00  2023-03-02 07:31:19  2023-03-02 07:31:19  2023      3   \n",
      "4  2023-02-27 23:00:00  2023-03-02 07:11:47  2023-03-02 07:11:47  2023      3   \n",
      "\n",
      "  month_name  day  day_name  hour  \n",
      "0      March    2  Thursday     9  \n",
      "1      March    2  Thursday     8  \n",
      "2      March    2  Thursday     7  \n",
      "3      March    2  Thursday     7  \n",
      "4      March    2  Thursday     7  \n",
      "\n",
      " load reviews: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 18:56:00,018 - INFO - Directories ensured: ../data/full, ../data/archive, ../parameters\n",
      "2025-03-26 18:56:00,018 - INFO - No existing file found at ../data/full/full_reviews.csv, initializing empty DataFrame\n",
      "2025-03-26 18:56:00,053 - INFO - Original records: 0\n",
      "2025-03-26 18:56:00,053 - INFO - Updated records: 50385\n",
      "2025-03-26 18:56:00,053 - INFO - New records added: 50385\n",
      "2025-03-26 18:56:00,054 - INFO - Wrote '1' to ../parameters/new_reviews.txt\n",
      "2025-03-26 18:56:00,055 - INFO - Percentage increase in data: 100.00%\n",
      "2025-03-26 18:56:00,241 - INFO - Updated data saved to ../data/full/full_reviews.csv\n",
      "2025-03-26 18:56:00,242 - INFO - Archived cleaned review file to: ../data/archive/cleaned_reviews_20250326185559.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50385 entries, 0 to 50384\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               50385 non-null  object\n",
      " 1   title            50385 non-null  object\n",
      " 2   review           50385 non-null  object\n",
      " 3   rating           50385 non-null  int64 \n",
      " 4   reply            7828 non-null   object\n",
      " 5   experienceDate   50385 non-null  object\n",
      " 6   createdDateTime  50385 non-null  object\n",
      " 7   publishedDate    50385 non-null  object\n",
      " 8   year             50385 non-null  int64 \n",
      " 9   month            50385 non-null  int64 \n",
      " 10  month_name       50385 non-null  object\n",
      " 11  day              50385 non-null  int64 \n",
      " 12  day_name         50385 non-null  object\n",
      " 13  hour             50385 non-null  int64 \n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 5.4+ MB\n",
      "None\n",
      "                         id  \\\n",
      "0  640053299b64b1bdaf6661e9   \n",
      "1  640046cda2e3a177e9e8b614   \n",
      "2  6400387bd143b326fe43200d   \n",
      "3  640034a7d143b326fe431f53   \n",
      "4  640030139b64b1bdaf665737   \n",
      "\n",
      "                                               title  \\\n",
      "0             Satisfait de mon choix avec backmarket   \n",
      "1                       A fuir, aucun service client   \n",
      "2                        Parfait,rapide,bien emballé   \n",
      "3                                             Pentax   \n",
      "4  Mécontent car produit endommagéBien que le col...   \n",
      "\n",
      "                                              review  rating reply  \\\n",
      "0                         satisfait choix backmarket       5   NaN   \n",
      "1  éviter reçu article demande retour fois avant ...       1   NaN   \n",
      "2                        parfait rapide bien emballé       5   NaN   \n",
      "3                              livraison très rapide       5   NaN   \n",
      "4  bien colis arrivé globalement décu batterie ar...       1   NaN   \n",
      "\n",
      "        experienceDate      createdDateTime        publishedDate  year  month  \\\n",
      "0  2023-02-28 23:00:00  2023-03-02 09:41:29  2023-03-02 09:41:29  2023      3   \n",
      "1  2023-03-01 00:00:00  2023-03-02 08:48:45  2023-03-02 08:48:45  2023      3   \n",
      "2  2023-02-28 23:00:00  2023-03-02 07:47:39  2023-03-02 07:47:39  2023      3   \n",
      "3  2023-02-28 23:00:00  2023-03-02 07:31:19  2023-03-02 07:31:19  2023      3   \n",
      "4  2023-02-27 23:00:00  2023-03-02 07:11:47  2023-03-02 07:11:47  2023      3   \n",
      "\n",
      "  month_name  day  day_name  hour  \n",
      "0      March    2  Thursday     9  \n",
      "1      March    2  Thursday     8  \n",
      "2      March    2  Thursday     7  \n",
      "3      March    2  Thursday     7  \n",
      "4      March    2  Thursday     7  \n"
     ]
    }
   ],
   "source": [
    "# application\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    raw_file_path = \"../data/raw/raw_reviews_0.csv\"\n",
    "    print(\"\\n transformed reviews: \\n\")\n",
    "    cleaned_file = process_reviews(raw_file_path)\n",
    "    print(f\"\\n cleaned_file_path: {cleaned_file} \\n\")\n",
    "\n",
    "    df_clean = pd.read_csv(cleaned_file)\n",
    "    print(df_clean.info())\n",
    "    print(df_clean.head())\n",
    "\n",
    "    print(\"\\n load reviews: \\n\")\n",
    "    load_reviews(cleaned_file)\n",
    "    df_full = pd.read_csv(\"../data/full/full_reviews.csv\")\n",
    "    print(df_full.info())\n",
    "    print(df_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "def load_reviews(cleaned_file) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes review data by merging new data with existing cleaned data.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_file (str): Path to the new review data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame after merging and deduplication\n",
    "        None: If an error occurs during processing or if no new data is added\n",
    "        \n",
    "    Key Features:\n",
    "        - Ensures output directory exists\n",
    "        - Handles existing data loading with error checking\n",
    "        - Updates data only if new reviews are added\n",
    "        - Writes \"1\" to ../parameters/new_reviews.txt if new reviews exist, otherwise \"0\"\n",
    "        - Logs percentage increase in data when new reviews are added\n",
    "        - Saves updated data with timestamp backup only if necessary\n",
    "        - Archives the cleaned review file after processing\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load new review data\n",
    "        df = pd.read_csv(cleaned_file)\n",
    "\n",
    "        # Define paths\n",
    "        base_dir = \"../data\"\n",
    "        full_reviews_folder = os.path.join(base_dir, \"full\")\n",
    "        archive_folder = os.path.join(base_dir, \"archive\")  # Archive folder for processed files\n",
    "        full_reviews_file = \"full_reviews.csv\"\n",
    "        full_reviews_path = os.path.join(full_reviews_folder, full_reviews_file)\n",
    "        new_reviews_flag_path = os.path.join(\"../parameters\", \"new_reviews.txt\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(full_reviews_folder, exist_ok=True)\n",
    "        os.makedirs(archive_folder, exist_ok=True)  # Create archive folder\n",
    "        os.makedirs(os.path.dirname(new_reviews_flag_path), exist_ok=True)\n",
    "        logging.info(f\"Directories ensured: {full_reviews_folder}, {archive_folder}, {os.path.dirname(new_reviews_flag_path)}\")\n",
    "\n",
    "        # Load existing data with error handling\n",
    "        df_full = pd.DataFrame()\n",
    "        if os.path.isfile(full_reviews_path):\n",
    "            try:\n",
    "                df_full = pd.read_csv(full_reviews_path, low_memory=False)\n",
    "                logging.info(f\"Loaded existing data from {full_reviews_path}\")\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.warning(f\"Empty CSV file found at {full_reviews_path}\")\n",
    "            except pd.errors.ParserError:\n",
    "                logging.error(f\"Parsing error in {full_reviews_path}\")\n",
    "        else:\n",
    "            logging.info(f\"No existing file found at {full_reviews_path}, initializing empty DataFrame\")\n",
    "\n",
    "        # Validate input DataFrame\n",
    "        if df.empty:\n",
    "            logging.warning(\"Input DataFrame is empty\")\n",
    "            return None\n",
    "\n",
    "        # Merge and deduplicate\n",
    "        initial_length = len(df_full)\n",
    "        df_full_updated = pd.concat([df_full, df], ignore_index=True)\n",
    "        df_full_updated = df_full_updated.drop_duplicates(subset=df.columns, keep='last')\n",
    "        final_length = len(df_full_updated)\n",
    "\n",
    "        # Log DataFrame sizes\n",
    "        logging.info(f\"Original records: {initial_length}\")\n",
    "        logging.info(f\"Updated records: {final_length}\")\n",
    "        new_records_added = final_length - initial_length\n",
    "        logging.info(f\"New records added: {new_records_added}\")\n",
    "\n",
    "        # Check if there are new records\n",
    "        has_new_reviews = final_length > initial_length\n",
    "        flag_value = \"1\" if has_new_reviews else \"0\"\n",
    "        with open(new_reviews_flag_path, \"w\") as f:\n",
    "            f.write(flag_value)\n",
    "        logging.info(f\"Wrote '{flag_value}' to {new_reviews_flag_path}\")\n",
    "\n",
    "        # Exit early if no new reviews are added\n",
    "        if not has_new_reviews:\n",
    "            logging.info(\"No new reviews to process. Exiting without updates.\")\n",
    "            return df_full_updated\n",
    "\n",
    "        # Calculate percentage increase\n",
    "        percentage_increase = ((final_length - initial_length) / initial_length * 100) if initial_length > 0 else 0\n",
    "        logging.info(f\"Percentage increase in data: {percentage_increase:.2f}%\")\n",
    "\n",
    "        # Create backup before overwriting\n",
    "        if os.path.isfile(full_reviews_path):\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_path = os.path.join(full_reviews_folder, f\"full_reviews_backup_{timestamp}.csv\")\n",
    "            df_full.to_csv(backup_path, index=False)\n",
    "            logging.info(f\"Backup created at {backup_path}\")\n",
    "\n",
    "        # Save updated data\n",
    "        df_full_updated.to_csv(full_reviews_path, index=False)\n",
    "        logging.info(f\"Updated data saved to {full_reviews_path}\")\n",
    "\n",
    "        # Archive the cleaned review file\n",
    "        try:\n",
    "            archive_path = os.path.join(archive_folder, os.path.basename(cleaned_file))\n",
    "            shutil.move(cleaned_file, archive_path)\n",
    "            logging.info(f\"Archived cleaned review file to: {archive_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error archiving cleaned review file: {e}\")\n",
    "\n",
    "        return df_full_updated\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing reviews: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50385 entries, 0 to 50384\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               50385 non-null  object\n",
      " 1   title            50385 non-null  object\n",
      " 2   review           50385 non-null  object\n",
      " 3   rating           50385 non-null  int64 \n",
      " 4   reply            7828 non-null   object\n",
      " 5   experienceDate   50385 non-null  object\n",
      " 6   createdDateTime  50385 non-null  object\n",
      " 7   publishedDate    50385 non-null  object\n",
      " 8   year             50385 non-null  int64 \n",
      " 9   month            50385 non-null  int64 \n",
      " 10  month_name       50385 non-null  object\n",
      " 11  day              50385 non-null  int64 \n",
      " 12  day_name         50385 non-null  object\n",
      " 13  hour             50385 non-null  int64 \n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 5.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>reply</th>\n",
       "      <th>experienceDate</th>\n",
       "      <th>createdDateTime</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>day</th>\n",
       "      <th>day_name</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640053299b64b1bdaf6661e9</td>\n",
       "      <td>Satisfait de mon choix avec backmarket</td>\n",
       "      <td>satisfait choix backmarket</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28 23:00:00</td>\n",
       "      <td>2023-03-02 09:41:29</td>\n",
       "      <td>2023-03-02 09:41:29</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640046cda2e3a177e9e8b614</td>\n",
       "      <td>A fuir, aucun service client</td>\n",
       "      <td>éviter reçu article demande retour fois avant ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01 00:00:00</td>\n",
       "      <td>2023-03-02 08:48:45</td>\n",
       "      <td>2023-03-02 08:48:45</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6400387bd143b326fe43200d</td>\n",
       "      <td>Parfait,rapide,bien emballé</td>\n",
       "      <td>parfait rapide bien emballé</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28 23:00:00</td>\n",
       "      <td>2023-03-02 07:47:39</td>\n",
       "      <td>2023-03-02 07:47:39</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>640034a7d143b326fe431f53</td>\n",
       "      <td>Pentax</td>\n",
       "      <td>livraison très rapide</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-28 23:00:00</td>\n",
       "      <td>2023-03-02 07:31:19</td>\n",
       "      <td>2023-03-02 07:31:19</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>640030139b64b1bdaf665737</td>\n",
       "      <td>Mécontent car produit endommagéBien que le col...</td>\n",
       "      <td>bien colis arrivé globalement décu batterie ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-27 23:00:00</td>\n",
       "      <td>2023-03-02 07:11:47</td>\n",
       "      <td>2023-03-02 07:11:47</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>2</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  640053299b64b1bdaf6661e9   \n",
       "1  640046cda2e3a177e9e8b614   \n",
       "2  6400387bd143b326fe43200d   \n",
       "3  640034a7d143b326fe431f53   \n",
       "4  640030139b64b1bdaf665737   \n",
       "\n",
       "                                               title  \\\n",
       "0             Satisfait de mon choix avec backmarket   \n",
       "1                       A fuir, aucun service client   \n",
       "2                        Parfait,rapide,bien emballé   \n",
       "3                                             Pentax   \n",
       "4  Mécontent car produit endommagéBien que le col...   \n",
       "\n",
       "                                              review  rating reply  \\\n",
       "0                         satisfait choix backmarket       5   NaN   \n",
       "1  éviter reçu article demande retour fois avant ...       1   NaN   \n",
       "2                        parfait rapide bien emballé       5   NaN   \n",
       "3                              livraison très rapide       5   NaN   \n",
       "4  bien colis arrivé globalement décu batterie ar...       1   NaN   \n",
       "\n",
       "        experienceDate      createdDateTime        publishedDate  year  month  \\\n",
       "0  2023-02-28 23:00:00  2023-03-02 09:41:29  2023-03-02 09:41:29  2023      3   \n",
       "1  2023-03-01 00:00:00  2023-03-02 08:48:45  2023-03-02 08:48:45  2023      3   \n",
       "2  2023-02-28 23:00:00  2023-03-02 07:47:39  2023-03-02 07:47:39  2023      3   \n",
       "3  2023-02-28 23:00:00  2023-03-02 07:31:19  2023-03-02 07:31:19  2023      3   \n",
       "4  2023-02-27 23:00:00  2023-03-02 07:11:47  2023-03-02 07:11:47  2023      3   \n",
       "\n",
       "  month_name  day  day_name  hour  \n",
       "0      March    2  Thursday     9  \n",
       "1      March    2  Thursday     8  \n",
       "2      March    2  Thursday     7  \n",
       "3      March    2  Thursday     7  \n",
       "4      March    2  Thursday     7  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/full/full_reviews.csv\")\n",
    "data.info()\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
