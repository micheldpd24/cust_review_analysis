{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Data Collection and Processing\n",
    "### Objectives :\n",
    "- Scrape \"Backmarket\" customers reviews from Trustpilot.\n",
    "- Clean review data collected\n",
    "- Processed reviews data collected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Logging Configuration\n",
    "- All log messages (INFO, ERROR, etc.) will be written to the specified log file (concatenation.log).\n",
    "- If StreamHandler is included, logs will also appear in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the logs directory exists\n",
    "log_dir = \"../logs_etl\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "log_file = os.path.join(log_dir, \"concatenation.log\")\n",
    "\n",
    "# Configure logging only if it hasn't been configured yet\n",
    "if not logging.root.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,  # Fixed logging level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),  # Write logs to a file\n",
    "            logging.StreamHandler()         # Optional: Keep console output\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "Target company for review analysis : Back Market - A global marketplace for refurbished devices. \n",
    "\n",
    "Review data will be collected from Trustpilot, a platform for collecting verified customer reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and parameters\n",
    "COMPANY_NAME = \"backmarket\" # Backmarket\n",
    "BASE_URL = f'https://fr.trustpilot.com/review/www.{COMPANY_NAME}.fr'\n",
    "MAX_PAGE_PARAM_FILE = Path(\"../parameters/max_page.txt\")\n",
    "NEW_REVIEW_PARAM_FILE = Path(\"../parameters/new_reviews.txt\")\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Directories\n",
    "RAW_DATA_DIR = Path(\"../data/raw\")\n",
    "CLEANED_DATA_DIR = Path(\"../data/cleaned\")\n",
    "FULL_REVIEWS_DIR = Path(\"../data/full\")\n",
    "ARCHIVE_DIR = Path(\"../data/archive\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to scrape reviews from trustpilot\n",
    "This Python function, extract_reviews, is designed to scrape and collect customer reviews from a paginated website ([trustpilot platform](https://fr.trustpilot.com/)) for a specific company. \n",
    "Here's a concise description of its functionality:\n",
    "\n",
    "1. **Purpose** :\n",
    "The function extracts structured review data (e.g., review ID, title, text, rating, reply, dates) from multiple pages of a website and saves the collected data into a CSV file.\n",
    "\n",
    "2. **Key Steps** :\n",
    "    - Input Parameters :\n",
    "        - company_name: Name of the company (default is a predefined constant).\n",
    "        - start_page: The page number to start scraping from (default is 1).\n",
    "        - end_page: The last page to scrape (default is a predefined constant).\n",
    "    - HTTP Requests :\n",
    "        Sends GET requests to the website using requests with custom headers and handles potential HTTP errors.\n",
    "    - HTML Parsing :\n",
    "        Uses BeautifulSoup to parse the HTML content and extract embedded JSON data containing reviews from a script tag.\n",
    "    - Data Extraction :\n",
    "        Iterates through the JSON data, extracting specific fields (e.g., review text, rating, dates) and storing them in a list of dictionaries.\n",
    "    -  Error Handling :\n",
    "        Includes error handling for network issues, missing data, and parsing errors.\n",
    "    - Rate Limiting :\n",
    "        Implements a delay (time.sleep) after processing every 10 pages to avoid overloading the server.\n",
    "    - Output :\n",
    "    Converts the collected data into a Pandas DataFrame and saves it as a CSV file in a specified directory.\n",
    "3. **Output** :\n",
    "    - Returns the file path of the saved CSV file containing the extracted reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of end_page is: 2150\n"
     ]
    }
   ],
   "source": [
    "# function to get the end page to collect reviews from\n",
    "def get_end_page(file_path=MAX_PAGE_PARAM_FILE):\n",
    "\n",
    "    try:\n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the first line and strip whitespace/newline characters\n",
    "            first_line = file.readline().strip()\n",
    "\n",
    "            # Validate that the line contains only digits\n",
    "            if first_line.isdigit():\n",
    "                return int(first_line)\n",
    "            else:\n",
    "                logging.error(\"Error: The first line does not contain a valid integer.\")\n",
    "                return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    end_page = get_end_page()\n",
    "    if end_page is not None:\n",
    "        print(f\"The value of end_page is: {end_page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract reviews for a given company name \n",
    "def extract_reviews(company_name=COMPANY_NAME):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from a paginated website for a specific company. \n",
    "    Extracts key review details such as text, rating, and dates into a structured format. \n",
    "    Saves the collected data as a CSV file for further analysis.\n",
    "    \"\"\"\n",
    "    headers = HEADERS\n",
    "    base_url = BASE_URL\n",
    "    keys = [\"id\", \"title\", \"review\", \"rating\", \"reply\", \"experienceDate\", \"createdDateTime\", \"publishedDate\", \"replyPublishedDate\"]\n",
    "    \n",
    "    new_max_page_flag_path = os.path.join(\"./parameters\", \"max_page.txt\")\n",
    "    end_page = get_end_page(file_path=new_max_page_flag_path)\n",
    "    end_page = max(10, end_page)\n",
    "    start_page = max(1, end_page - 10)\n",
    "\n",
    "    reviews_list = []  # List to collect all review data\n",
    "    counter = 0\n",
    "    for page in range(end_page, start_page, -1):\n",
    "        counter += 1\n",
    "        logging.info(f\"Processing page {page}\")\n",
    "        \n",
    "        url_page = f\"{base_url}?page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url_page, headers=headers, timeout=5)\n",
    "            response.raise_for_status()  # Verify HTTP errors\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error accessing page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            script_content = soup.body.script.contents if soup.body and soup.body.script else None\n",
    "            \n",
    "            if not script_content:\n",
    "                logging.warning(f\"No data found in page {page}\")\n",
    "                continue\n",
    "\n",
    "            raw_data = json.loads(script_content[0])\n",
    "            raw_data = raw_data.get(\"props\", {}).get(\"pageProps\", {}).get(\"reviews\", [])\n",
    "            \n",
    "            for review in raw_data:\n",
    "                tmp = {}\n",
    "                tmp[\"id\"] = review.get(\"id\")\n",
    "                tmp[\"title\"] = review.get(\"title\")\n",
    "                tmp[\"review\"] = review.get(\"text\")\n",
    "                tmp[\"rating\"] = review.get(\"rating\")\n",
    "                try:\n",
    "                    tmp[\"reply\"] = review.get(\"reply\", {}).get(\"message\")\n",
    "                    tmp[\"replyPublishedDate\"] = review.get(\"reply\", {}).get(\"publishedDate\")\n",
    "                except:\n",
    "                    tmp[\"reply\"] = None\n",
    "                    tmp[\"replyPublishedDate\"] = None\n",
    "                \n",
    "                tmp[\"experienceDate\"] = review.get(\"dates\", {}).get(\"experiencedDate\")\n",
    "                tmp[\"createdDateTime\"] = review.get(\"labels\", {}).get(\"verification\", {}).get(\"createdDateTime\")\n",
    "                tmp[\"publishedDate\"] = review.get(\"dates\", {}).get(\"publishedDate\")\n",
    "\n",
    "                reviews_list.append({key: tmp.get(key) for key in keys})\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing page {page}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Avoid hitting the server too frequently\n",
    "        # if counter % 5 == 0 and counter != 10:\n",
    "        #     logging.info(\"Sleeping for 45 seconds to avoid overloading the server.\")\n",
    "        #     time.sleep(45)\n",
    "\n",
    "    if not reviews_list:\n",
    "        logging.warning(\"No reviews collected.\")\n",
    "        return\n",
    "    \n",
    "    # Convert list of dicts to DataFrame and save to CSV\n",
    "    df_raw_reviews = pd.DataFrame(reviews_list)\n",
    "\n",
    "    # Save reviews data into a CSV file\n",
    "    output_dir = \"../data/raw\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    raw_file_path = os.path.join(output_dir, f\"raw_reviews_{start_page+1}-{end_page}.csv\")\n",
    "    df_raw_reviews.to_csv(raw_file_path, index=False)\n",
    "    logging.info(f\"Saved reviews data to {raw_file_path}\")\n",
    "    \n",
    "    # Update the next max page flag\n",
    "    if start_page >= 10:\n",
    "        flag_value = start_page\n",
    "    else: \n",
    "        flag_value = 10\n",
    "    with open(new_max_page_flag_path, \"w\") as f:\n",
    "        f.write(str(flag_value))\n",
    "    logging.info(f\"Wrote new max page value '{flag_value}' to {new_max_page_flag_path}\")\n",
    "    \n",
    "    logging.info(f\"Finished processing pages {start_page}-{end_page}\")\n",
    "    \n",
    "    return raw_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data collection\n",
    "There are over 3,000 review pages for Back Market, making it time-consuming to scrape all the data. To avoid overloading the server, we ran the extraction function in batches of 25 pages, incorporating significant delays between batches. The initial scraped data has been saved in the \"data/raw\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all extracted reviews into one csv file\n",
    "def concatenate_reviews(input_dir=\"../data/raw\", \\\n",
    "                        output_file=\"concat_reviews.csv\", \\\n",
    "                        file_prefix=\"raw_reviews_\"):\n",
    "    \"\"\"\n",
    "    Concatenates multiple review CSV files into a single DataFrame,\n",
    "    removes duplicates and missing values, \n",
    "    saves the cleaned data to a new CSV file,\n",
    "    and moves processed files to an '.archive' folder.\n",
    "    \"\"\"\n",
    "    # Validate input directory\n",
    "    if not os.path.exists(input_dir):\n",
    "        logging.error(f\"Input directory '{input_dir}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # List files matching the prefix\n",
    "    files = [f for f in os.listdir(input_dir) if f.startswith(file_prefix)]\n",
    "    if not files:\n",
    "        logging.error(f\"No files found with prefix '{file_prefix}' in '{input_dir}'.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Found {len(files)} files to process.\")\n",
    "\n",
    "    # Create .archive folder if it doesn't exist\n",
    "    archive_dir = os.path.join(input_dir, \".archive\")\n",
    "    os.makedirs(archive_dir, exist_ok=True)\n",
    "    logging.info(f\"Created/verified '.archive' folder at: {archive_dir}\")\n",
    "\n",
    "    # Read and concatenate files efficiently\n",
    "    try:\n",
    "        df_list = []\n",
    "        for f in files:\n",
    "            file_path = os.path.join(input_dir, f)\n",
    "            logging.info(f\"Reading file: {file_path}\")\n",
    "            df_list.append(pd.read_csv(file_path))\n",
    "        \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        logging.info(\"Concatenated all files into a single DataFrame.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading or concatenating files: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Log initial state\n",
    "    logging.info(f\"Initial DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "    logging.info(f\"Removed duplicates. New DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Drop rows with missing values in critical columns\n",
    "    critical_columns = [\"id\", \"review\", \"rating\", \"experienceDate\"]\n",
    "    df.dropna(subset=critical_columns, inplace=True)\n",
    "    logging.info(f\"Removed rows with missing values. Final DataFrame info:\\n{df.info()}\")\n",
    "\n",
    "    # Save cleaned DataFrame to CSV\n",
    "    try:\n",
    "        output_path = os.path.join(input_dir, output_file)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Saved cleaned DataFrame to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cleaned DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Move processed files to .archive folder\n",
    "    try:\n",
    "        for f in files:\n",
    "            src_path = os.path.join(input_dir, f)\n",
    "            dst_path = os.path.join(archive_dir, f)\n",
    "            shutil.move(src_path, dst_path)\n",
    "            logging.info(f\"Moved file to archive: {dst_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error moving files to archive: {e}\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24522 entries, 0 to 24521\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     24522 non-null  object \n",
      " 1   title                  24522 non-null  object \n",
      " 2   review                 24522 non-null  object \n",
      " 3   rating                 24522 non-null  int64  \n",
      " 4   reply                  5897 non-null   object \n",
      " 5   experienceDate         24522 non-null  object \n",
      " 6   createdDateTime        24522 non-null  object \n",
      " 7   publishedDate          24522 non-null  object \n",
      " 8   replyPublishedDate     5897 non-null   object \n",
      " 9   reviewExperienceDelay  24522 non-null  float64\n",
      " 10  date                   24522 non-null  object \n",
      " 11  year                   24522 non-null  int64  \n",
      " 12  yearQuarter            24522 non-null  object \n",
      " 13  yearMonth              24522 non-null  object \n",
      " 14  month                  24522 non-null  int64  \n",
      " 15  monthName              24522 non-null  object \n",
      " 16  day                    24522 non-null  int64  \n",
      " 17  dayName                24522 non-null  object \n",
      " 18  hour                   24522 non-null  int64  \n",
      " 19  replyYear              5897 non-null   float64\n",
      " 20  replyMonth             5897 non-null   float64\n",
      " 21  replyDay               5897 non-null   float64\n",
      " 22  replyHour              5897 non-null   float64\n",
      " 23  reviewLength           24522 non-null  int64  \n",
      " 24  titleLength            24522 non-null  int64  \n",
      " 25  sentiment              24522 non-null  object \n",
      "dtypes: float64(5), int64(7), object(14)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/full/full_reviews.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 15:38:59,097 - INFO - Found 25 files to process.\n",
      "2025-03-30 15:38:59,099 - INFO - Created/verified '.archive' folder at: ../data/raw/.archive\n",
      "2025-03-30 15:38:59,100 - INFO - Reading file: ../data/raw/raw_reviews_2251-2260.csv\n",
      "2025-03-30 15:38:59,106 - INFO - Reading file: ../data/raw/raw_reviews_2211-2220.csv\n",
      "2025-03-30 15:38:59,110 - INFO - Reading file: ../data/raw/raw_reviews_1871-1880.csv\n",
      "2025-03-30 15:38:59,114 - INFO - Reading file: ../data/raw/raw_reviews_2291-2300.csv\n",
      "2025-03-30 15:38:59,118 - INFO - Reading file: ../data/raw/raw_reviews_2181-2190.csv\n",
      "2025-03-30 15:38:59,120 - INFO - Reading file: ../data/raw/raw_reviews_2271-2280.csv\n",
      "2025-03-30 15:38:59,122 - INFO - Reading file: ../data/raw/raw_reviews_2161-2170.csv\n",
      "2025-03-30 15:38:59,124 - INFO - Reading file: ../data/raw/raw_reviews_2861-2870.csv\n",
      "2025-03-30 15:38:59,127 - INFO - Reading file: ../data/raw/raw_reviews_2301-2310.csv\n",
      "2025-03-30 15:38:59,128 - INFO - Reading file: ../data/raw/raw_reviews_1851-1860.csv\n",
      "2025-03-30 15:38:59,131 - INFO - Reading file: ../data/raw/raw_reviews_1861-1870.csv\n",
      "2025-03-30 15:38:59,133 - INFO - Reading file: ../data/raw/raw_reviews_2201-2210.csv\n",
      "2025-03-30 15:38:59,135 - INFO - Reading file: ../data/raw/raw_reviews_1831-1840.csv\n",
      "2025-03-30 15:38:59,136 - INFO - Reading file: ../data/raw/raw_reviews_2241-2250.csv\n",
      "2025-03-30 15:38:59,138 - INFO - Reading file: ../data/raw/raw_reviews_.csv\n",
      "2025-03-30 15:38:59,203 - INFO - Reading file: ../data/raw/raw_reviews_1821-1830.csv\n",
      "2025-03-30 15:38:59,205 - INFO - Reading file: ../data/raw/raw_reviews_2151-2160.csv\n",
      "2025-03-30 15:38:59,206 - INFO - Reading file: ../data/raw/raw_reviews_1841-1850.csv\n",
      "2025-03-30 15:38:59,208 - INFO - Reading file: ../data/raw/raw_reviews_2311-2320.csv\n",
      "2025-03-30 15:38:59,210 - INFO - Reading file: ../data/raw/raw_reviews_2221-2230.csv\n",
      "2025-03-30 15:38:59,211 - INFO - Reading file: ../data/raw/raw_reviews_2261-2270.csv\n",
      "2025-03-30 15:38:59,212 - INFO - Reading file: ../data/raw/raw_reviews_2281-2290.csv\n",
      "2025-03-30 15:38:59,214 - INFO - Reading file: ../data/raw/raw_reviews_2171-2180.csv\n",
      "2025-03-30 15:38:59,216 - INFO - Reading file: ../data/raw/raw_reviews_2191-2200.csv\n",
      "2025-03-30 15:38:59,217 - INFO - Reading file: ../data/raw/raw_reviews_2231-2240.csv\n",
      "2025-03-30 15:38:59,220 - INFO - Concatenated all files into a single DataFrame.\n",
      "2025-03-30 15:38:59,223 - INFO - Initial DataFrame info:\n",
      "None\n",
      "2025-03-30 15:38:59,230 - INFO - Removed duplicates. New DataFrame info:\n",
      "None\n",
      "2025-03-30 15:38:59,237 - INFO - Removed rows with missing values. Final DataFrame info:\n",
      "None\n",
      "2025-03-30 15:38:59,338 - INFO - Saved cleaned DataFrame to: ../data/raw/concat_reviews.csv\n",
      "2025-03-30 15:38:59,339 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2251-2260.csv\n",
      "2025-03-30 15:38:59,340 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2211-2220.csv\n",
      "2025-03-30 15:38:59,341 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1871-1880.csv\n",
      "2025-03-30 15:38:59,341 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2291-2300.csv\n",
      "2025-03-30 15:38:59,342 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2181-2190.csv\n",
      "2025-03-30 15:38:59,342 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2271-2280.csv\n",
      "2025-03-30 15:38:59,342 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2161-2170.csv\n",
      "2025-03-30 15:38:59,343 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2861-2870.csv\n",
      "2025-03-30 15:38:59,343 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2301-2310.csv\n",
      "2025-03-30 15:38:59,344 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1851-1860.csv\n",
      "2025-03-30 15:38:59,344 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1861-1870.csv\n",
      "2025-03-30 15:38:59,344 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2201-2210.csv\n",
      "2025-03-30 15:38:59,345 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1831-1840.csv\n",
      "2025-03-30 15:38:59,345 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2241-2250.csv\n",
      "2025-03-30 15:38:59,345 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_.csv\n",
      "2025-03-30 15:38:59,346 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1821-1830.csv\n",
      "2025-03-30 15:38:59,346 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2151-2160.csv\n",
      "2025-03-30 15:38:59,346 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_1841-1850.csv\n",
      "2025-03-30 15:38:59,347 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2311-2320.csv\n",
      "2025-03-30 15:38:59,347 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2221-2230.csv\n",
      "2025-03-30 15:38:59,347 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2261-2270.csv\n",
      "2025-03-30 15:38:59,348 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2281-2290.csv\n",
      "2025-03-30 15:38:59,348 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2171-2180.csv\n",
      "2025-03-30 15:38:59,349 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2191-2200.csv\n",
      "2025-03-30 15:38:59,349 - INFO - Moved file to archive: ../data/raw/.archive/raw_reviews_2231-2240.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25162 entries, 0 to 25161\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  25162 non-null  object\n",
      " 1   title               25162 non-null  object\n",
      " 2   review              25162 non-null  object\n",
      " 3   rating              25162 non-null  int64 \n",
      " 4   reply               5966 non-null   object\n",
      " 5   experienceDate      25151 non-null  object\n",
      " 6   createdDateTime     25162 non-null  object\n",
      " 7   publishedDate       25162 non-null  object\n",
      " 8   replyPublishedDate  5966 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.7+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24574 entries, 0 to 25161\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  24574 non-null  object\n",
      " 1   title               24574 non-null  object\n",
      " 2   review              24574 non-null  object\n",
      " 3   rating              24574 non-null  int64 \n",
      " 4   reply               5898 non-null   object\n",
      " 5   experienceDate      24563 non-null  object\n",
      " 6   createdDateTime     24574 non-null  object\n",
      " 7   publishedDate       24574 non-null  object\n",
      " 8   replyPublishedDate  5898 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24563 entries, 0 to 25161\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  24563 non-null  object\n",
      " 1   title               24563 non-null  object\n",
      " 2   review              24563 non-null  object\n",
      " 3   rating              24563 non-null  int64 \n",
      " 4   reply               5889 non-null   object\n",
      " 5   experienceDate      24563 non-null  object\n",
      " 6   createdDateTime     24563 non-null  object\n",
      " 7   publishedDate       24563 non-null  object\n",
      " 8   replyPublishedDate  5889 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24563 entries, 0 to 25161\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  24563 non-null  object\n",
      " 1   title               24563 non-null  object\n",
      " 2   review              24563 non-null  object\n",
      " 3   rating              24563 non-null  int64 \n",
      " 4   reply               5889 non-null   object\n",
      " 5   experienceDate      24563 non-null  object\n",
      " 6   createdDateTime     24563 non-null  object\n",
      " 7   publishedDate       24563 non-null  object\n",
      " 8   replyPublishedDate  5889 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_concat = concatenate_reviews()\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process reviews data\n",
    "\n",
    "In this step we preprocess raw text data for Natural Language Processing (NLP) tasks by performing a series of cleaning and normalization steps. We removes noise (e.g., hashtags, URLs, mentions, stopwords), converts text to lowercase, tokenizes it, filters out non-alphabetic tokens, and returns the cleaned text as a single string.\n",
    "\n",
    "#### Text cleaning and normalization\n",
    "- Noise Removal :\n",
    "    - Removes hashtags, HTML entities, stock tickers, URLs, retweet tags, mentions, and special characters.\n",
    "    - Replaces emojis with their textual descriptions and strips punctuation.\n",
    "\n",
    "- Normalization :\n",
    "    - Converts text to lowercase, replaces ampersands (&) with \"and,\" and removes short words (â‰¤3 characters).\n",
    "\n",
    "- Tokenization :\n",
    "    - Splits text into tokens using French-specific word tokenization.\n",
    "\n",
    "- Filtering :\n",
    "    - Removes numbers, non-alphabetic tokens, and stopwords (including custom additions to the French stopwords list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love this product great fast service\n"
     ]
    }
   ],
   "source": [
    "# Function to clean and process review text\n",
    "\n",
    "# os.environ[\"NLTK_DATA\"] = \"/Users/micheldpd/Projects/custrev/nltk_data\"\n",
    "\n",
    "# Preload stopwords outside the function to avoid repeated loading\n",
    "STOP_WORDS_TO_ADD = [\"Ãªtre\", \"leur\", \"leurs\", \"avoir\", \"cela\", \"les\", \"de\", \"pour\", \"des\", \"cette\", \"a\",\n",
    "                   \"j'ai\", \"car\", \"c'est\", \"chez\", \"tout\", \"fait\", \"chez\", \"donc\", \n",
    "                   \"n'est\", \"si\", \"alors\", \"n'ai\", \"faire\", \"deux\", \"comme\", \"jour\", \"tr\", \"si\", \"ue\",\n",
    "                   \"back\", \"market\", \"backmarket\"\n",
    "\n",
    "]\n",
    "STOP_WORDS = set(stopwords.words('french')).union(set(STOP_WORDS_TO_ADD))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing noise (e.g., hashtags, URLs, stopwords) and normalizing content.\n",
    "    Tokenizes, filters alphabetic tokens, and removes French stopwords for NLP tasks.\n",
    "    Returns the cleaned and normalized text as a single string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove hashtags (keep text after #)\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Remove HTML special entities (e.g., &amp;)\n",
    "    text = re.sub(r'&\\w*;', '', text)\n",
    "\n",
    "    # Remove stock tickers (e.g., $AAPL)\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "\n",
    "    # Remove hyperlinks (covers various URL patterns)\n",
    "    text = re.sub(r'https?://[^\\s/$.?#].[^\\s]*', '', text)\n",
    "    text = re.sub(r'http(\\S)+', '', text)  # Catch incomplete URLs\n",
    "    text = re.sub(r'http\\s*\\.\\.\\.', '', text)  # Catch truncated URLs\n",
    "\n",
    "    # Remove retweet tags and mentions\n",
    "    text = re.sub(r'(RT|rt)\\s*@\\s*\\S+', '', text)\n",
    "    text = re.sub(r'RT\\s?@', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "    # Replace & with 'and', fix < and > (assuming intent was to escape them)\n",
    "    text = re.sub(r'&', 'and', text)\n",
    "\n",
    "    # Remove words with 3 or fewer letters (e.g., \"the\", \"cat\")\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', ' ', text)\n",
    "\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode\n",
    "    text = ''.join(c for c in text if ord(c) <= 0xFFFF)\n",
    "\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert emojis to text descriptions (e.g., ðŸ˜Š -> :smiling_face:)\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove punctuation, keeping alphanumeric characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Tokenize text (lowercase for consistency)\n",
    "    tokens: List[str] = word_tokenize(text.lower(), language='french')\n",
    "\n",
    "    # Filter out numbers and keep only alphabetic tokens\n",
    "    tokens_alpha = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens_cleaned = [token for token in tokens_alpha if token not in STOP_WORDS]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens_cleaned)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"RT @user: I love this product! < and 16Â§789> #great https://example.com ðŸ˜Š &amp; fast service\"\n",
    "    cleaned = clean_text(sample_text)\n",
    "    print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review data cleaning and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define customer sentiment\n",
    "- Customer sentiment is \"positive\" if rating >= 4\n",
    "- Customer sentiment is neutral if rating == 3\n",
    "- Customer sentiment is \"negative if rating < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the sentiment base on the rating value\n",
    "\n",
    "def get_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return \"positive\"\n",
    "    elif rating == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean, process and enrich raw review data\n",
    "def process_reviews(raw_file):\n",
    "    try:\n",
    "        df = pd.read_csv(raw_file)\n",
    "        logging.info(f\"Successfully loaded {len(df)} rows of data.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading raw data: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df[\"experienceDate\"] = pd.to_datetime(df[\"experienceDate\"])\n",
    "        df[\"experienceDate\"] = df[\"experienceDate\"].dt.tz_localize(None)\n",
    "        df[\"createdDateTime\"] = pd.to_datetime(df[\"createdDateTime\"])\n",
    "        df[\"createdDateTime\"] = df[\"createdDateTime\"].dt.tz_localize(None)\n",
    "        df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "        df[\"publishedDate\"] = df[\"publishedDate\"].dt.tz_localize(None)\n",
    "        df[\"reviewExperienceDelay\"] = (df[\"createdDateTime\"] - df[\"experienceDate\"]).dt.total_seconds() / 60\n",
    "        try:\n",
    "            df[\"replyPublishedDate\"] = pd.to_datetime(df[\"replyPublishedDate\"])\n",
    "        except:\n",
    "            df[\"replyPublishedDate\"] = None\n",
    "        logging.info(\"Date formats standardized successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error standardizing date formats: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.date\n",
    "        df[\"year\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.year\n",
    "        df[\"yearQuarter\"] = (\n",
    "            pd.to_datetime(df[\"createdDateTime\"]).dt.year.astype(str)\n",
    "            + \"-Q\"\n",
    "            + pd.to_datetime(df[\"createdDateTime\"]).dt.quarter.astype(str)\n",
    "        )\n",
    "        df[\"yearMonth\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.strftime(\"%Y\" +\"-\"+ \"%m\")\n",
    "\n",
    "        df[\"month\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.month\n",
    "        df[\"monthName\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.month_name()\n",
    "        df[\"day\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.day\n",
    "        df[\"dayName\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.day_name()\n",
    "        df[\"hour\"] = pd.to_datetime(df[\"createdDateTime\"]).dt.hour\n",
    "        try:\n",
    "            df[\"replyYear\"] = pd.to_datetime(df[\"replyPublishedDate\"]).dt.year\n",
    "            df[\"replyMonth\"] = pd.to_datetime(df[\"replyPublishedDate\"]).dt.month\n",
    "            df[\"replyDay\"] = pd.to_datetime(df[\"replyPublishedDate\"]).dt.day\n",
    "            df[\"replyHour\"] = pd.to_datetime(df[\"replyPublishedDate\"]).dt.hour\n",
    "        except:\n",
    "            df[\"replyYear\"] = None\n",
    "            df[\"replyMonth\"] = None\n",
    "            df[\"replyDay\"] = None\n",
    "            df[\"replyHour\"] = None\n",
    "        logging.info(\"Temporal features extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting temporal features: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Add columns for review length and number of words\n",
    "    try:\n",
    "        df[\"reviewLength\"] = df[\"review\"].str.len()\n",
    "        df[\"titleLength\"] = df[\"title\"].str.len()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error adding review length column: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        initial_rows = len(df)\n",
    "        df.dropna(\n",
    "            inplace=True,\n",
    "            subset=[\"id\", \"review\", \"rating\", \"experienceDate\", \"createdDateTime\", \"publishedDate\"],\n",
    "        )\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} rows with missing values. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing rows with missing values: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"rating\"].isin([1, 2, 3, 4, 5])]\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} rows with invalid ratings. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing rows with invalid ratings: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df[\"sentiment\"] = df[\"rating\"].apply(lambda x: \"positive\" if x >= 4 else (\"neutral\" if x == 3 else \"negative\"))\n",
    "        logging.info(\"Sentiment column added successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error adding sentiment column: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df[\"review\"] = df[\"review\"].apply(clean_text)\n",
    "        logging.info(\"Review text cleaned successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning review text: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"review\"].str.len() > 4]\n",
    "        removed_rows = initial_rows - len(df)\n",
    "        logging.info(f\"Removed {removed_rows} short reviews. Remaining rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error removing short reviews: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        output_dir = CLEANED_DATA_DIR\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        cleaned_file_path = output_dir / f\"cleaned_reviews_{timestamp}.csv\"\n",
    "        df.to_csv(cleaned_file_path, index=False)\n",
    "        logging.info(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cleaned data: {e}\")\n",
    "        return None\n",
    "\n",
    "    return str(cleaned_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 15:41:24,685 - INFO - Successfully loaded 24563 rows of data.\n",
      "2025-03-30 15:41:24,723 - INFO - Date formats standardized successfully.\n",
      "2025-03-30 15:41:24,820 - INFO - Temporal features extracted successfully.\n",
      "2025-03-30 15:41:24,831 - INFO - Removed 0 rows with missing values. Remaining rows: 24563\n",
      "2025-03-30 15:41:24,834 - INFO - Removed 0 rows with invalid ratings. Remaining rows: 24563\n",
      "2025-03-30 15:41:24,836 - INFO - Sentiment column added successfully.\n",
      "2025-03-30 15:41:26,940 - INFO - Review text cleaned successfully.\n",
      "2025-03-30 15:41:26,946 - INFO - Removed 59 short reviews. Remaining rows: 24504\n",
      "2025-03-30 15:41:27,130 - INFO - Cleaned data saved to: ../data/cleaned/cleaned_reviews_20250330154126.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cleaned/cleaned_reviews_20250330154126.csv\n"
     ]
    }
   ],
   "source": [
    "# Processed raw review\n",
    "# file_path = process_reviews(\"../data/raw/concat_reviews.csv\")\n",
    "# print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading cleaned review into the full review base\n",
    "\n",
    "After cleaning and processing, the extracted raw reviews are consolidated into the full review database.\n",
    "Before consolidation, the pipeline checks for the presence of new data. If no new data is found, the full review database remains unchanged, and the pipeline stops at this stage.\n",
    "Once the loading process is complete, the cleaned review file is archived to ensure proper organization and to maintain a history of processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load cleaned and process reviews data into the review database\n",
    "\n",
    "# Named constants for return values\n",
    "SUCCESS = 1\n",
    "NO_UPDATES = 0\n",
    "ERROR = None\n",
    "\n",
    "def ensure_directory_exists(path: Path):\n",
    "    \"\"\"Ensure the given directory exists.\"\"\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_file_path(directory: Path, filename: str) -> Path:\n",
    "    \"\"\"Construct a file path from a directory and filename.\"\"\"\n",
    "    return directory / filename\n",
    "\n",
    "def calculate_percentage_increase(initial: int, final: int) -> float:\n",
    "    \"\"\"Calculate the percentage increase between two values.\"\"\"\n",
    "    return 100 if initial == 0 else ((final - initial) / initial * 100)\n",
    "\n",
    "def load_reviews(cleaned_file: str, full_reviews_dir: Path = FULL_REVIEWS_DIR, archive_dir: Path = ARCHIVE_DIR) -> int:\n",
    "    \"\"\"\n",
    "    Load cleaned reviews into the full database.\n",
    "\n",
    "    Args:\n",
    "        cleaned_file (str): Path to the cleaned reviews CSV file.\n",
    "        full_reviews_dir (Path): Directory for the full reviews file.\n",
    "        archive_dir (Path): Directory for archived files.\n",
    "\n",
    "    Returns:\n",
    "        int: SUCCESS (1) if new records were added, NO_UPDATES (0) if no updates were made, ERROR (None) if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure directories exist\n",
    "        ensure_directory_exists(full_reviews_dir)\n",
    "        ensure_directory_exists(archive_dir)\n",
    "\n",
    "        # Load cleaned reviews\n",
    "        df = pd.read_csv(cleaned_file)\n",
    "        if df.empty:\n",
    "            logging.warning(\"Input DataFrame is empty\")\n",
    "            return NO_UPDATES\n",
    "\n",
    "        # Define file paths\n",
    "        full_reviews_path = get_file_path(full_reviews_dir, \"full_reviews.csv\")\n",
    "        df_full = pd.DataFrame()\n",
    "\n",
    "        # Load existing data if it exists\n",
    "        if full_reviews_path.exists():\n",
    "            try:\n",
    "                df_full = pd.read_csv(full_reviews_path, low_memory=False)\n",
    "                logging.info(f\"Loaded existing data from {full_reviews_path}\")\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.warning(f\"Empty CSV file found at {full_reviews_path}\")\n",
    "            except pd.errors.ParserError:\n",
    "                logging.error(f\"Parsing error in {full_reviews_path}\")\n",
    "        else:\n",
    "            logging.info(f\"No existing file found at {full_reviews_path}, initializing empty DataFrame\")\n",
    "\n",
    "        # Update full reviews\n",
    "        initial_length = len(df_full)\n",
    "        df_full_updated = pd.concat([df_full, df], ignore_index=True).drop_duplicates(subset=df.columns, keep=\"last\")\n",
    "        final_length = len(df_full_updated)\n",
    "        new_records_added = final_length - initial_length\n",
    "\n",
    "        logging.info(f\"Original records: {initial_length}\")\n",
    "        logging.info(f\"Updated records: {final_length}\")\n",
    "        logging.info(f\"New records added: {new_records_added}\")\n",
    "\n",
    "        if new_records_added > 0:\n",
    "            percentage_increase = calculate_percentage_increase(initial_length, final_length)\n",
    "            logging.info(f\"Percentage increase in data: {percentage_increase:.2f}%\")\n",
    "\n",
    "            # Backup existing data\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_path = get_file_path(full_reviews_dir, f\"full_reviews_backup_{timestamp}.csv\")\n",
    "            df_full.to_csv(backup_path, index=False)\n",
    "            logging.info(f\"Backup created at {backup_path}\")\n",
    "\n",
    "            # Save updated data\n",
    "            df_full_updated.to_csv(full_reviews_path, index=False)\n",
    "            logging.info(f\"Updated data saved to {full_reviews_path}\")\n",
    "\n",
    "            return SUCCESS\n",
    "        else:\n",
    "            logging.info(\"No new reviews to process. Exiting without updates.\")\n",
    "            return NO_UPDATES\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading reviews: {e}\")\n",
    "        return ERROR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 15:43:11,419 - INFO - No existing file found at ../data/full/full_reviews.csv, initializing empty DataFrame\n",
      "2025-03-30 15:43:11,440 - INFO - Original records: 0\n",
      "2025-03-30 15:43:11,441 - INFO - Updated records: 24504\n",
      "2025-03-30 15:43:11,441 - INFO - New records added: 24504\n",
      "2025-03-30 15:43:11,441 - INFO - Percentage increase in data: 100.00%\n",
      "2025-03-30 15:43:11,442 - INFO - Backup created at ../data/full/full_reviews_backup_20250330_154311.csv\n",
      "2025-03-30 15:43:11,575 - INFO - Updated data saved to ../data/full/full_reviews.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Application\n",
    "# result = load_reviews(file_path)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 09:11:52,163 - INFO - Processing page 3210\n",
      "2025-03-27 09:11:52,661 - INFO - Processing page 3209\n",
      "2025-03-27 09:11:53,188 - INFO - Processing page 3208\n",
      "2025-03-27 09:11:53,711 - INFO - Processing page 3207\n",
      "2025-03-27 09:11:54,236 - INFO - Processing page 3206\n",
      "2025-03-27 09:11:54,792 - INFO - Processing page 3205\n",
      "2025-03-27 09:11:55,878 - INFO - Processing page 3204\n",
      "2025-03-27 09:11:57,394 - INFO - Processing page 3203\n",
      "2025-03-27 09:11:57,905 - INFO - Processing page 3202\n",
      "2025-03-27 09:11:58,437 - INFO - Processing page 3201\n",
      "2025-03-27 09:11:58,963 - INFO - Saved reviews data to ../data/raw/raw_reviews_3201-3210.csv\n",
      "2025-03-27 09:11:58,964 - INFO - Wrote new max page value '3200' to ../parameters/max_page.txt\n",
      "2025-03-27 09:11:58,964 - INFO - Finished processing pages 3200-3210\n",
      "2025-03-27 09:11:58,964 - INFO - Loading raw data from: ../data/raw/raw_reviews_3201-3210.csv\n",
      "2025-03-27 09:11:58,967 - INFO - Successfully loaded 200 rows of data.\n",
      "2025-03-27 09:11:58,967 - INFO - Standardizing date formats...\n",
      "2025-03-27 09:11:58,969 - INFO - Date formats standardized successfully.\n",
      "2025-03-27 09:11:58,970 - INFO - Extracting temporal features (year, month, day, hour)...\n",
      "2025-03-27 09:11:58,973 - INFO - Temporal features extracted successfully.\n",
      "2025-03-27 09:11:58,973 - INFO - Removing rows with missing values...\n",
      "2025-03-27 09:11:58,975 - INFO - Removed 2 rows with missing values. Remaining rows: 198\n",
      "2025-03-27 09:11:58,975 - INFO - Removing rows with invalid ratings...\n",
      "2025-03-27 09:11:58,976 - INFO - Removed 0 rows with invalid ratings. Remaining rows: 198\n",
      "2025-03-27 09:11:58,976 - INFO - Adding sentiment column...\n",
      "2025-03-27 09:11:58,976 - INFO - Sentiment column added successfully.\n",
      "2025-03-27 09:11:58,977 - INFO - Cleaning review text...\n",
      "2025-03-27 09:11:58,998 - INFO - Review text cleaned successfully.\n",
      "2025-03-27 09:11:58,999 - INFO - Removing short reviews...\n",
      "2025-03-27 09:11:58,999 - INFO - Removed 0 short reviews. Remaining rows: 198\n",
      "2025-03-27 09:11:59,000 - INFO - Saving cleaned data to CSV...\n",
      "2025-03-27 09:11:59,003 - INFO - Cleaned data saved to: ../data/cleaned/cleaned_reviews_20250327091159.csv\n",
      "2025-03-27 09:11:59,009 - INFO - Directories ensured: ../data/full, ../data/archive, ../parameters\n",
      "2025-03-27 09:11:59,013 - INFO - Loaded existing data from ../data/full/full_reviews.csv\n",
      "2025-03-27 09:11:59,015 - INFO - Original records: 779\n",
      "2025-03-27 09:11:59,015 - INFO - Updated records: 977\n",
      "2025-03-27 09:11:59,015 - INFO - New records added: 198\n",
      "2025-03-27 09:11:59,016 - INFO - Wrote '1' to ../parameters/new_reviews.txt\n",
      "2025-03-27 09:11:59,016 - INFO - Percentage increase in data: 25.42%\n",
      "2025-03-27 09:11:59,021 - INFO - Backup created at ../data/full/full_reviews_backup_20250327_091159.csv\n",
      "2025-03-27 09:11:59,026 - INFO - Updated data saved to ../data/full/full_reviews.csv\n",
      "2025-03-27 09:11:59,026 - INFO - Archived cleaned review file to: ../data/archive/cleaned_reviews_20250327091159.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cleaned_file_path: ../data/cleaned/cleaned_reviews_20250327091159.csv \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198 entries, 0 to 197\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     198 non-null    object \n",
      " 1   title                  198 non-null    object \n",
      " 2   review                 198 non-null    object \n",
      " 3   rating                 198 non-null    int64  \n",
      " 4   reply                  17 non-null     object \n",
      " 5   experienceDate         198 non-null    object \n",
      " 6   createdDateTime        198 non-null    object \n",
      " 7   publishedDate          198 non-null    object \n",
      " 8   replyPublishedDate     17 non-null     object \n",
      " 9   reviewExperienceDelay  198 non-null    float64\n",
      " 10  year                   198 non-null    int64  \n",
      " 11  yearQuarter            198 non-null    object \n",
      " 12  month                  198 non-null    int64  \n",
      " 13  monthName              198 non-null    object \n",
      " 14  day                    198 non-null    int64  \n",
      " 15  dayName                198 non-null    object \n",
      " 16  hour                   198 non-null    int64  \n",
      " 17  replyYear              17 non-null     float64\n",
      " 18  replyMonth             17 non-null     float64\n",
      " 19  replyDay               17 non-null     float64\n",
      " 20  replyHour              17 non-null     float64\n",
      " 21  sentiment              198 non-null    object \n",
      "dtypes: float64(5), int64(5), object(12)\n",
      "memory usage: 34.2+ KB\n",
      "None\n",
      "                         id                                   title  \\\n",
      "0  5aef45056d33bc09d016596e    Merci pour le matÃ©riel en excellentâ€¦   \n",
      "1  5aef39156d33bc09d01656d0   Le tÃ©lÃ©phone fonctionne impec mais...   \n",
      "2  5aeed0696d33bc05347a13be                       TrÃ¨s  bon produit   \n",
      "3  5aeecd936d33bc09d0164186            TÃ©lÃ©phone en mauvais Ã©tat!!!   \n",
      "4  5aeecd616d33bc05347a1324  Recommande fortement - Mieux que prÃ©vu   \n",
      "\n",
      "                                              review  rating  \\\n",
      "0             merci matÃ©riel excellent Ã©tat rapiditÃ©       5   \n",
      "1  tÃ©lÃ©phone fonctionne impec transaction rapide ...       3   \n",
      "2                  trÃ¨s produit conforme description       4   \n",
      "3  dÃ©lais livraison respectes malheureusement tÃ©l...       2   \n",
      "4  service livrÃ© plus vite prÃ©vu malgrÃ© indicatio...       5   \n",
      "\n",
      "                                               reply  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  Bonjour Emilie,\\n\\nNavrÃ© d'apprendre cela :(\\n...   \n",
      "4                                                NaN   \n",
      "\n",
      "              experienceDate            createdDateTime  \\\n",
      "0  2018-05-06 18:10:12+00:00  2018-05-06 18:10:12+00:00   \n",
      "1  2018-05-06 17:19:17+00:00  2018-05-06 17:19:17+00:00   \n",
      "2  2018-05-06 09:52:41+00:00  2018-05-06 09:52:41+00:00   \n",
      "3  2018-05-06 00:00:00+00:00  2018-05-06 09:40:35+00:00   \n",
      "4  2018-05-06 09:39:44+00:00  2018-05-06 09:39:44+00:00   \n",
      "\n",
      "               publishedDate                replyPublishedDate  \\\n",
      "0  2018-05-06 18:10:12+00:00                               NaN   \n",
      "1  2018-05-06 17:19:17+00:00                               NaN   \n",
      "2  2018-05-06 09:52:41+00:00                               NaN   \n",
      "3  2018-05-06 09:40:35+00:00  2018-05-09 16:23:08.965000+00:00   \n",
      "4  2018-05-06 09:39:44+00:00                               NaN   \n",
      "\n",
      "   reviewExperienceDelay  ...  month monthName  day dayName  hour replyYear  \\\n",
      "0               0.000000  ...      5       May    6  Sunday    18       NaN   \n",
      "1               0.000000  ...      5       May    6  Sunday    17       NaN   \n",
      "2               0.000000  ...      5       May    6  Sunday     9       NaN   \n",
      "3             580.583333  ...      5       May    6  Sunday     9    2018.0   \n",
      "4               0.000000  ...      5       May    6  Sunday     9       NaN   \n",
      "\n",
      "   replyMonth  replyDay  replyHour  sentiment  \n",
      "0         NaN       NaN        NaN   positive  \n",
      "1         NaN       NaN        NaN    neutral  \n",
      "2         NaN       NaN        NaN   positive  \n",
      "3         5.0       9.0       16.0   negative  \n",
      "4         NaN       NaN        NaN   positive  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      " load reviews: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 977 entries, 0 to 976\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     977 non-null    object \n",
      " 1   title                  977 non-null    object \n",
      " 2   review                 977 non-null    object \n",
      " 3   rating                 977 non-null    int64  \n",
      " 4   reply                  97 non-null     object \n",
      " 5   experienceDate         977 non-null    object \n",
      " 6   createdDateTime        977 non-null    object \n",
      " 7   publishedDate          977 non-null    object \n",
      " 8   replyPublishedDate     97 non-null     object \n",
      " 9   reviewExperienceDelay  977 non-null    float64\n",
      " 10  year                   977 non-null    int64  \n",
      " 11  yearQuarter            977 non-null    object \n",
      " 12  month                  977 non-null    int64  \n",
      " 13  monthName              977 non-null    object \n",
      " 14  day                    977 non-null    int64  \n",
      " 15  dayName                977 non-null    object \n",
      " 16  hour                   977 non-null    int64  \n",
      " 17  replyYear              97 non-null     float64\n",
      " 18  replyMonth             97 non-null     float64\n",
      " 19  replyDay               97 non-null     float64\n",
      " 20  replyHour              97 non-null     float64\n",
      " 21  sentiment              977 non-null    object \n",
      "dtypes: float64(5), int64(5), object(12)\n",
      "memory usage: 168.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# application\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     raw_file_path = extract_reviews(company_name=COMPANY_NAME)\n",
    "#     cleaned_file = process_reviews(raw_file_path)\n",
    "#     print(f\"\\n cleaned_file_path: {cleaned_file} \\n\")\n",
    "\n",
    "#     df_clean = pd.read_csv(cleaned_file)\n",
    "#     print(df_clean.info())\n",
    "#     print(df_clean.head())\n",
    "\n",
    "#     print(\"\\n load reviews: \\n\")\n",
    "#     load_reviews(cleaned_file)\n",
    "#     df_full = pd.read_csv(\"../data/full/full_reviews.csv\")\n",
    "#     df_full.info()\n",
    "#     df_full.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
